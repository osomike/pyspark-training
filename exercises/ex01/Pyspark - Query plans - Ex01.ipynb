{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# PySpark Training Notebook\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "####  Run these cells to configure your interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%idle_timeout 30\n",
    "%glue_version 5.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"--enable-spark-ui\": \"true\",\n",
    "    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n",
    "    \"--enable-metrics\": \"true\",\n",
    "    \"--enable-observability-metrics\": \"true\",\n",
    "    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n",
    "    \"--conf\": \"spark.sql.ui.retainedStages=100\",\n",
    "    \"--conf\": \"spark.sql.codegen.comments=true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get spark's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\n",
    "dynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\n",
    "dynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\n",
    "dynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n",
    "\n",
    "executor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\n",
    "executor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\n",
    "executor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n",
    "\n",
    "driver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\n",
    "driver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n",
    "\n",
    "print(f'''\n",
    "Dynamic allocation enabled: {dynamic_allocation_enabled}\n",
    "Dynamic min executors: {dynamic_min_executors}\n",
    "Dynamic max executors: {dynamic_max_executors}\n",
    "Dynamic initial executors: {dynamic_initial_executors}\n",
    "----------------------------------------\n",
    "Executor instances: {executor_instances}\n",
    "Executor cores: {executor_cores}\n",
    "Executor memory: {executor_memory}\n",
    "----------------------------------------\n",
    "Driver cores: {driver_cores}\n",
    "Driver memory: {driver_memory}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the New York's taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/big/ny-taxi-dataset/')\n",
    "p_df = spark.read.format('parquet').load('s3://dip-pyspark-training//data/big/ny-taxi-dataset-partitioned/')\n",
    "#df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#p_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.filter(F.col('vendor_id') == 'VTS').explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('surcharge_amount', F.col('total_amount') * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('is_long_trip', F.col('trip_distance') > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df4 = df3.withColumn('trip_category', F.when(F.col('passenger_count') <= 2, F.lit('small group')).when(F.col('passenger_count') <= 4, F.lit('medium group')).otherwise(F.lit('big group')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df5 = df4.filter(F.col('vendor_id') == 'VTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df6 = df5.select(['vendor_id', 'total_amount', 'surcharge_amount', 'trip_distance', 'is_long_trip', 'passenger_count', 'trip_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df6.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "output_file_path_non_partitioned = 's3://dip-pyspark-training/output/dummy-output-01'\n",
    "df6.write.mode('overwrite').format('parquet').save(output_file_path_non_partitioned)\n",
    "pt = (datetime.datetime.now() - ts).seconds\n",
    "print(f'The processing time was {pt} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_non_partitioned = spark.read.format('parquet').load(output_file_path_non_partitioned).count()\n",
    "c_non_partitioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a partitioned source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_df2 = p_df.withColumn('surcharge_amount', F.col('total_amount') * 0.1)\n",
    "p_df3 = p_df2.withColumn('is_long_trip', F.col('trip_distance') > 10)\n",
    "p_df4 = p_df3.withColumn('trip_category', F.when(F.col('passenger_count') <= 2, F.lit('small group')).when(F.col('passenger_count') <= 4, F.lit('medium group')).otherwise(F.lit('big group')))\n",
    "p_df5 = p_df4.filter(F.col('vendor_id') == 'VTS')\n",
    "p_df6 = p_df5.select(['vendor_id', 'total_amount', 'surcharge_amount', 'trip_distance', 'is_long_trip', 'passenger_count', 'trip_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_df6.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "output_file_path_partitioned = 's3://dip-pyspark-training/output/dummy-output-02'\n",
    "p_df6.write.mode('overwrite').format('parquet').save(output_file_path_partitioned)\n",
    "p_pt = (datetime.datetime.now() - ts).seconds\n",
    "print(f'The processing time was {p_pt} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_partitioned = spark.read.format('parquet').load(output_file_path_partitioned).count()\n",
    "c_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert c_non_partitioned == c_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = round(100 * (pt - p_pt)/p_pt, 2)\n",
    "print(f'Using the partitioned source was {d}% faster than the non-partitioned one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
