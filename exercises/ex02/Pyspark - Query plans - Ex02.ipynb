{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# PySpark Training Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run these cells to configure your interactive session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 30\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 4",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 30 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--enable-continuous-cloudwatch-log\": \"true\",\n    \"--enable-spark-ui\": \"true\",\n    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n    \"--enable-metrics\": \"true\",\n    \"--enable-observability-metrics\": \"true\",\n    \"--conf\": \"spark.sql.codegen.comments=true\",\n    \"--conf\": \"spark.sql.codegen.fallback=true\",\n    \"--conf\": \"spark.sql.codegen.wholeStage=true\",\n    \"--conf\": \"spark.sql.ui.explainMode=extended\",\n    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n    \"--conf\": \"spark.ui.retainedJobs=1000\",\n    \"--conf\": \"spark.ui.retainedStages=1000\",\n    \"--conf\": \"spark.ui.retainedTasks=10000\",\n    \"--conf\": \"spark.ui.showAdditionalMetrics=true\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--enable-continuous-cloudwatch-log': 'true', '--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://dip-pyspark-training/spark_ui_tmp/', '--enable-metrics': 'true', '--enable-observability-metrics': 'true', '--conf': 'spark.ui.showAdditionalMetrics=true'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Start spark session ",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 4\nIdle Timeout: 30\nSession ID: a2c53a3a-2ab6-455f-a5e0-0c7bbe25a5ba\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--enable-continuous-cloudwatch-log true\n--enable-spark-ui true\n--spark-event-logs-path s3://dip-pyspark-training/spark_ui_tmp/\n--enable-metrics true\n--enable-observability-metrics true\n--conf spark.ui.showAdditionalMetrics=true\nWaiting for session a2c53a3a-2ab6-455f-a5e0-0c7bbe25a5ba to get into ready status...\nSession a2c53a3a-2ab6-455f-a5e0-0c7bbe25a5ba has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Get spark's configuration",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\ndynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\ndynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\ndynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n\nexecutor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\nexecutor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\nexecutor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n\ndriver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\ndriver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n\nprint(f'''\nDynamic allocation enabled: {dynamic_allocation_enabled}\nDynamic min executors: {dynamic_min_executors}\nDynamic max executors: {dynamic_max_executors}\nDynamic initial executors: {dynamic_initial_executors}\n----------------------------------------\nExecutor instances: {executor_instances}\nExecutor cores: {executor_cores}\nExecutor memory: {executor_memory}\n----------------------------------------\nDriver cores: {driver_cores}\nDriver memory: {driver_memory}\n''')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\nDynamic allocation enabled: false\nDynamic min executors: 1\nDynamic max executors: 3\nDynamic initial executors: 3\n----------------------------------------\nExecutor instances: 3\nExecutor cores: 4\nExecutor memory: 10g\n----------------------------------------\nDriver cores: 4\nDriver memory: 10g\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Import libraries",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport datetime",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Loading the New York's taxi dataset",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# partitioned file\np_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/big/ny-taxi-dataset-partitioned/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# p_df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Define the data as lists\nvendors = ['VTS', 'CMT', 'DDS', 'VTS', 'CMT', 'DDS']\npayment_type = ['CASH', 'CASH', 'CASH', 'CREDIT', 'CREDIT', 'CREDIT']\nextra_col = ['A', 'B', 'C', 'D', 'E', 'F']\n\n# Define the schema of the dataframe\nschema = T.StructType([\n    T.StructField(\"vendor_id\", T.StringType(), False),\n    T.StructField(\"payment_type\", T.StringType(), False),\n    T.StructField(\"extra_col_from_m\", T.StringType(), False)\n])\n\n# Create a list of tuples\ndata = [(vendors[i], payment_type[i], extra_col[i]) for i in range(len(vendors))]\n\n# Create a PySpark dataframe\nm_df = spark.createDataFrame(data, schema)\n# m_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------------+----------------+\n|vendor_id|payment_type|extra_col_from_m|\n+---------+------------+----------------+\n|      VTS|        CASH|               A|\n|      CMT|        CASH|               B|\n|      DDS|        CASH|               C|\n|      VTS|      CREDIT|               D|\n|      CMT|      CREDIT|               E|\n|      DDS|      CREDIT|               F|\n+---------+------------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# to_join_location = 's3://dip-pyspark-training/data/big/to_join_data/'\n# m_df.write.format('parquet').mode('overwrite').save(to_join_location)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark_application_id = spark.sparkContext.applicationId.split('-')[-1]\n# tmp_table_name = f'{spark_application_id}_tmp_table'\n# tmp_table_name",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "'1736866877970_tmp_table'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Register table on the spark catalog\n# spark.sql(f\"\"\"\n# CREATE TABLE {tmp_table_name} (\n#     vendor_id STRING,\n#     payment_type STRING,\n#     extra_col_from_m STRING\n# )\n# STORED AS PARQUET\n# LOCATION '{to_join_location}'\n# \"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Make sure we obtain the metadata needed to fetch the size of this table only\n# spark.sql(f'ANALYZE TABLE {tmp_table_name} COMPUTE STATISTICS')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# m_df_from_catalog = spark.sql(f'SELECT * FROM {tmp_table_name}')\n# m_df_from_catalog.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------------+----------------+\n|vendor_id|payment_type|extra_col_from_m|\n+---------+------------+----------------+\n|      VTS|      CREDIT|               D|\n|      CMT|      CREDIT|               E|\n|      DDS|      CREDIT|               F|\n|      VTS|        CASH|               A|\n|      CMT|        CASH|               B|\n|      DDS|        CASH|               C|\n+---------+------------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# READ: spark.sql.autoBroadcastJoinThreshold https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options\n# READ: Join Hints https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#join-hints-types\n# p_joined_df = p_df.join(other=m_df, how='inner', on = ['vendor_id', 'payment_type'])\np_joined_df = p_df.join(other=m_df.hint('broadcast'), how='inner', on = ['vendor_id', 'payment_type'])\n# p_joined_df = p_df.join(other=m_df_from_catalog, how='inner', on = ['vendor_id', 'payment_type'])\np_joined_df.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Join UsingJoin(Inner, [vendor_id, payment_type])\n:- Relation [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] parquet\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [vendor_id#36, payment_type#37, extra_col_from_m#38], false\n\n== Analyzed Logical Plan ==\nvendor_id: string, payment_type: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, trip_distance: double, pickup_longitude: double, pickup_latitude: double, rate_code_id: int, store_and_fwd_flag: string, dropoff_longitude: double, dropoff_latitude: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, total_amount: double, extra_col_from_m: string\nProject [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n+- Join Inner, ((vendor_id#17 = vendor_id#36) AND (payment_type#10 = payment_type#37))\n   :- Relation [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] parquet\n   +- ResolvedHint (strategy=broadcast)\n      +- LogicalRDD [vendor_id#36, payment_type#37, extra_col_from_m#38], false\n\n== Optimized Logical Plan ==\nProject [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n+- Join Inner, ((vendor_id#17 = vendor_id#36) AND (payment_type#10 = payment_type#37)), rightHint=(strategy=broadcast)\n   :- Filter (isnotnull(vendor_id#17) AND isnotnull(payment_type#10))\n   :  +- Relation [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] parquet\n   +- LogicalRDD [vendor_id#36, payment_type#37, extra_col_from_m#38], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n   +- BroadcastHashJoin [vendor_id#17, payment_type#10], [vendor_id#36, payment_type#37], Inner, BuildRight, false\n      :- Filter isnotnull(payment_type#10)\n      :  +- FileScan parquet [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] Batched: true, DataFilters: [isnotnull(payment_type#10)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/big/ny-taxi-dataset-partitioned], PartitionFilters: [isnotnull(vendor_id#17)], PushedFilters: [IsNotNull(payment_type)], ReadSchema: struct<pickup_datetime:timestamp,dropoff_datetime:timestamp,passenger_count:int,trip_distance:dou...\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=161]\n         +- Scan ExistingRDD[vendor_id#36,payment_type#37,extra_col_from_m#38]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ts = datetime.datetime.now()\noutput_file_path_partitioned = 's3://dip-pyspark-training/output/merged-dataset-02/'\np_joined_df.write.format('parquet').mode('overwrite').save(output_file_path_partitioned)\np_pt = (datetime.datetime.now() - ts).seconds\nprint(f'The processing time was {p_pt} seconds')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "The processing time was 150 seconds\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# p_tmp_df = spark.read.format('parquet').load(output_file_path_partitioned)\n# p_tmp_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+----------------+\n|vendor_id|payment_type|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|extra_col_from_m|\n+---------+------------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+----------------+\n|      VTS|        CASH|2009-12-25 18:42:00|2009-12-25 18:57:00|              5|         3.52|       73.978847|      40.761798|        NULL|              NULL|       -74.006057|       40.741337|       11.3|  0.0|    0.5|       0.0|         0.0|        11.8|               A|\n|      VTS|        CASH|2009-12-06 17:22:00|2009-12-06 17:40:00|              1|         1.06|        73.97086|       40.75884|        NULL|              NULL|       -73.984793|       40.758985|       10.1|  0.0|    0.5|       0.0|         0.0|        10.6|               A|\n|      VTS|        CASH|2009-06-06 19:33:00|2009-06-06 19:39:00|              1|         1.37|      -74.005285|       40.72867|        NULL|              NULL|       -74.014508|       40.715452|        5.7|  0.0|   NULL|       0.0|         0.0|         5.7|               A|\n|      VTS|        CASH|2009-06-03 10:54:00|2009-06-03 11:00:00|              1|         0.99|      -74.001072|      40.718445|        NULL|              NULL|        -74.00376|       40.707888|        5.3|  0.0|   NULL|       0.0|         0.0|         5.3|               A|\n|      VTS|        CASH|2009-06-04 20:08:00|2009-06-04 20:25:00|              2|         3.74|      -74.008403|      40.734962|        NULL|              NULL|       -73.963608|       40.755985|       12.1|  0.5|   NULL|       0.0|         0.0|        12.6|               A|\n+---------+------------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+----------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# p_tmp_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "71448102\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "'10485760b'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.conf.get(\"spark.sql.join.preferSortMergeJoin\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "'false'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# drop the temporaty table\n# spark.sql(f'DROP TABLE {tmp_table_name}')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		}
	]
}