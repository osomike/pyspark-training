{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# PySpark Training Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run these cells to configure your interactive session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 30\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 4",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 30 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--enable-continuous-cloudwatch-log\": \"true\",\n    \"--enable-spark-ui\": \"true\",\n    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n    \"--enable-metrics\": \"true\",\n    \"--enable-observability-metrics\": \"true\",\n    \"--conf\": \"spark.sql.codegen.comments=true\",\n    \"--conf\": \"spark.sql.codegen.fallback=true\",\n    \"--conf\": \"spark.sql.codegen.wholeStage=true\",\n    \"--conf\": \"spark.sql.ui.explainMode=extended\",\n    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n    \"--conf\": \"spark.ui.retainedJobs=1000\",\n    \"--conf\": \"spark.ui.retainedStages=1000\",\n    \"--conf\": \"spark.ui.retainedTasks=10000\",\n    \"--conf\": \"spark.ui.showAdditionalMetrics=true\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--enable-continuous-cloudwatch-log': 'true', '--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://dip-pyspark-training/spark_ui_tmp/', '--enable-metrics': 'true', '--enable-observability-metrics': 'true', '--conf': 'spark.ui.showAdditionalMetrics=true'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Start spark session ",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 4\nIdle Timeout: 30\nSession ID: dcb2903d-99ca-4042-a120-524785e0fb6d\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--enable-continuous-cloudwatch-log true\n--enable-spark-ui true\n--spark-event-logs-path s3://dip-pyspark-training/spark_ui_tmp/\n--enable-metrics true\n--enable-observability-metrics true\n--conf spark.ui.showAdditionalMetrics=true\nWaiting for session dcb2903d-99ca-4042-a120-524785e0fb6d to get into ready status...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Get spark's configuration",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\ndynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\ndynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\ndynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n\nexecutor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\nexecutor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\nexecutor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n\ndriver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\ndriver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n\nprint(f'''\nDynamic allocation enabled: {dynamic_allocation_enabled}\nDynamic min executors: {dynamic_min_executors}\nDynamic max executors: {dynamic_max_executors}\nDynamic initial executors: {dynamic_initial_executors}\n----------------------------------------\nExecutor instances: {executor_instances}\nExecutor cores: {executor_cores}\nExecutor memory: {executor_memory}\n----------------------------------------\nDriver cores: {driver_cores}\nDriver memory: {driver_memory}\n''')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Import libraries",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport datetime",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Loading the New York's taxi dataset",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# partitioned file\np_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/big/ny-taxi-dataset-partitioned/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# p_df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Define the data as lists\nvendors = ['VTS', 'CMT', 'DDS', 'VTS', 'CMT', 'DDS']\npayment_type = ['CASH', 'CASH', 'CASH', 'CREDIT', 'CREDIT', 'CREDIT']\nextra_col = ['A', 'B', 'C', 'D', 'E', 'F']\n\n# Define the schema of the dataframe\nschema = T.StructType([\n    T.StructField(\"vendor_id\", T.StringType(), False),\n    T.StructField(\"payment_type\", T.StringType(), False),\n    T.StructField(\"extra_col_from_m\", T.StringType(), False)\n])\n\n# Create a list of tuples\ndata = [(vendors[i], payment_type[i], extra_col[i]) for i in range(len(vendors))]\n\n# Create a PySpark dataframe\nm_df = spark.createDataFrame(data, schema)\n# m_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# to_join_location = 's3://dip-pyspark-training/data/big/to_join_data/'\n# m_df.write.format('parquet').mode('overwrite').save(to_join_location)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark_application_id = spark.sparkContext.applicationId.split('-')[-1]\n# tmp_table_name = f'{spark_application_id}_tmp_table'\n# tmp_table_name",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Register table on the spark catalog\n# spark.sql(f\"\"\"\n# CREATE TABLE {tmp_table_name} (\n#     vendor_id STRING,\n#     payment_type STRING,\n#     extra_col_from_m STRING\n# )\n# STORED AS PARQUET\n# LOCATION '{to_join_location}'\n# \"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Make sure we obtain the metadata needed to fetch the size of this table only\n# spark.sql(f'ANALYZE TABLE {tmp_table_name} COMPUTE STATISTICS')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# m_df_from_catalog = spark.sql(f'SELECT * FROM {tmp_table_name}')\n# m_df_from_catalog.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "p_joined_df = p_df.join(other=m_df, how='inner', on = ['vendor_id', 'payment_type'])\n# p_joined_df = p_df.join(other=m_df.hint('broadcast'), how='inner', on = ['vendor_id', 'payment_type'])\n# p_joined_df = p_df.join(other=m_df_from_catalog, how='inner', on = ['vendor_id', 'payment_type'])\np_joined_df.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "ts = datetime.datetime.now()\noutput_file_path_partitioned = 's3://dip-pyspark-training/output/merged-dataset-02/'\np_joined_df.write.format('parquet').mode('overwrite').save(output_file_path_partitioned)\np_pt = (datetime.datetime.now() - ts).seconds\nprint(f'The processing time was {p_pt} seconds')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "The processing time was 150 seconds\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# p_tmp_df = spark.read.format('parquet').load(output_file_path_partitioned)\n# p_tmp_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+----------------+\n|vendor_id|payment_type|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|extra_col_from_m|\n+---------+------------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+----------------+\n|      VTS|        CASH|2009-12-25 18:42:00|2009-12-25 18:57:00|              5|         3.52|       73.978847|      40.761798|        NULL|              NULL|       -74.006057|       40.741337|       11.3|  0.0|    0.5|       0.0|         0.0|        11.8|               A|\n|      VTS|        CASH|2009-12-06 17:22:00|2009-12-06 17:40:00|              1|         1.06|        73.97086|       40.75884|        NULL|              NULL|       -73.984793|       40.758985|       10.1|  0.0|    0.5|       0.0|         0.0|        10.6|               A|\n|      VTS|        CASH|2009-06-06 19:33:00|2009-06-06 19:39:00|              1|         1.37|      -74.005285|       40.72867|        NULL|              NULL|       -74.014508|       40.715452|        5.7|  0.0|   NULL|       0.0|         0.0|         5.7|               A|\n|      VTS|        CASH|2009-06-03 10:54:00|2009-06-03 11:00:00|              1|         0.99|      -74.001072|      40.718445|        NULL|              NULL|        -74.00376|       40.707888|        5.3|  0.0|   NULL|       0.0|         0.0|         5.3|               A|\n|      VTS|        CASH|2009-06-04 20:08:00|2009-06-04 20:25:00|              2|         3.74|      -74.008403|      40.734962|        NULL|              NULL|       -73.963608|       40.755985|       12.1|  0.5|   NULL|       0.0|         0.0|        12.6|               A|\n+---------+------------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+----------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# p_tmp_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "71448102\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "'10485760b'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.conf.get(\"spark.sql.join.preferSortMergeJoin\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "'false'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# drop the temporaty table\n# spark.sql(f'DROP TABLE {tmp_table_name}')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		}
	]
}