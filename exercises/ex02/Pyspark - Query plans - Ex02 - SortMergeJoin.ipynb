{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# PySpark Training Notebook\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Run these cells to configure your interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%idle_timeout 30\n",
    "%glue_version 5.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"--enable-continuous-cloudwatch-log\": \"true\",\n",
    "    \"--enable-spark-ui\": \"true\",\n",
    "    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n",
    "    \"--enable-metrics\": \"true\",\n",
    "    \"--enable-observability-metrics\": \"true\",\n",
    "    \"--conf\": \"spark.sql.codegen.comments=true\",\n",
    "    \"--conf\": \"spark.sql.codegen.fallback=true\",\n",
    "    \"--conf\": \"spark.sql.codegen.wholeStage=true\",\n",
    "    \"--conf\": \"spark.sql.ui.explainMode=extended\",\n",
    "    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n",
    "    \"--conf\": \"spark.ui.retainedJobs=1000\",\n",
    "    \"--conf\": \"spark.ui.retainedStages=1000\",\n",
    "    \"--conf\": \"spark.ui.retainedTasks=10000\",\n",
    "    \"--conf\": \"spark.ui.showAdditionalMetrics=true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Start spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Get spark's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\n",
    "dynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\n",
    "dynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\n",
    "dynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n",
    "\n",
    "executor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\n",
    "executor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\n",
    "executor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n",
    "\n",
    "driver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\n",
    "driver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n",
    "\n",
    "print(f'''\n",
    "Dynamic allocation enabled: {dynamic_allocation_enabled}\n",
    "Dynamic min executors: {dynamic_min_executors}\n",
    "Dynamic max executors: {dynamic_max_executors}\n",
    "Dynamic initial executors: {dynamic_initial_executors}\n",
    "----------------------------------------\n",
    "Executor instances: {executor_instances}\n",
    "Executor cores: {executor_cores}\n",
    "Executor memory: {executor_memory}\n",
    "----------------------------------------\n",
    "Driver cores: {driver_cores}\n",
    "Driver memory: {driver_memory}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the New York's taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# partitioned file\n",
    "p_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/big/ny-taxi-dataset-partitioned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the data as lists\n",
    "vendors = ['VTS', 'CMT', 'DDS', 'VTS', 'CMT', 'DDS']\n",
    "payment_type = ['CASH', 'CASH', 'CASH', 'CREDIT', 'CREDIT', 'CREDIT']\n",
    "extra_col = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "# Define the schema of the dataframe\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"vendor_id\", T.StringType(), False),\n",
    "    T.StructField(\"payment_type\", T.StringType(), False),\n",
    "    T.StructField(\"extra_col_from_m\", T.StringType(), False)\n",
    "])\n",
    "\n",
    "# Create a list of tuples\n",
    "data = [(vendors[i], payment_type[i], extra_col[i]) for i in range(len(vendors))]\n",
    "\n",
    "# Create a PySpark dataframe\n",
    "m_df = spark.createDataFrame(data, schema)\n",
    "# m_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to_join_location = 's3://dip-pyspark-training/data/big/to_join_data/'\n",
    "# m_df.write.format('parquet').mode('overwrite').save(to_join_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark_application_id = spark.sparkContext.applicationId.split('-')[-1]\n",
    "# tmp_table_name = f'{spark_application_id}_tmp_table'\n",
    "# tmp_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register table on the spark catalog\n",
    "# spark.sql(f\"\"\"\n",
    "# CREATE TABLE {tmp_table_name} (\n",
    "#     vendor_id STRING,\n",
    "#     payment_type STRING,\n",
    "#     extra_col_from_m STRING\n",
    "# )\n",
    "# STORED AS PARQUET\n",
    "# LOCATION '{to_join_location}'\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure we obtain the metadata needed to fetch the size of this table only\n",
    "# spark.sql(f'ANALYZE TABLE {tmp_table_name} COMPUTE STATISTICS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m_df_from_catalog = spark.sql(f'SELECT * FROM {tmp_table_name}')\n",
    "# m_df_from_catalog.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_joined_df = p_df.join(other=m_df, how='inner', on = ['vendor_id', 'payment_type'])\n",
    "# p_joined_df = p_df.join(other=m_df.hint('broadcast'), how='inner', on = ['vendor_id', 'payment_type'])\n",
    "# p_joined_df = p_df.join(other=m_df_from_catalog, how='inner', on = ['vendor_id', 'payment_type'])\n",
    "p_joined_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "output_file_path_partitioned = 's3://dip-pyspark-training/output/merged-dataset-02/'\n",
    "p_joined_df.write.format('parquet').mode('overwrite').save(output_file_path_partitioned)\n",
    "p_pt = (datetime.datetime.now() - ts).seconds\n",
    "print(f'The processing time was {p_pt} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p_tmp_df = spark.read.format('parquet').load(output_file_path_partitioned)\n",
    "# p_tmp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p_tmp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.sql.join.preferSortMergeJoin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the temporaty table\n",
    "# spark.sql(f'DROP TABLE {tmp_table_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
