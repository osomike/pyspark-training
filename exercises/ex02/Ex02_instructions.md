# Exercise 02: Exploring Joins with PySpark

In this exercise, you will work with the New York Taxi dataset and a smaller dataset to explore and understand joins in PySpark. You will perform various join operations and analyze their effects on the data.

You will:
- Load the New York Taxi dataset from specified S3 paths.
- Create a smaller dataset to be used in the join.
- Perform an inner join between the two datasets on common columns.
- Analyze the query plans generated by Spark for the join operation.
- Measure the performance of the join operation and compare the results.

## Objectives

1. **Load Data**: Load the New York Taxi dataset from the specified S3 path.
2. **Load Data**: Create a smaller dataset with the following information:


<p align="center">

| vendor_id | payment_type | extra_col_from_m |
|-----------|--------------|------------------|
| VTS       | CASH         | A                |
| CMT       | CASH         | B                |
| DDS       | CASH         | C                |
| VTS       | CREDIT       | D                |
| CMT       | CREDIT       | E                |
| DDS       | CREDIT       | F                |

</p>


3. **Perform Join**: Perform an inner join between the two datasets on common columns such as `vendor_id` and `payment_type`.
4. **Query Plans**: Use the `explain` method to analyze the query plans for your join operation.
5. **Performance Measurement**: Measure the time taken for the join operation and compare the results.

## Instructions

1. **Setup the Environment**: 
    - Start an AWS Glue Interactive Session with the following configuration:
      ```python
      %idle_timeout 30
      %glue_version 5.0
      %worker_type G.1X
      %number_of_workers 4
      ```

2. **Load Data**:
    - Load the New York Taxi dataset from the specified S3 path.
    - Create the smaller dataset using the information provided above.

3. **Perform Join**:
    - Perform an inner join between the two datasets on the common columns `vendor_id` and `payment_type`.

4. **Query Plans**:
    - Use the `explain` method to analyze the query plans for your join operation.
    - Observe the logical and physical plans generated by Spark.

5. **Performance Measurement**:
    - Measure the time taken for the join operation. Determine if the operation completed successfully. If it did not, analyze the issues encountered. Consider alternative join strategies that could be used to improve performance.

    Tips:
    - [Types of joins](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#join-hints)
    - [Autobroadcast join threshold](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options)
    - [Analyze table command](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-analyze-table.html)


By the end of this exercise, you should have a deeper understanding of join operations in PySpark and how to analyze and optimize your Spark jobs.
