# PySpark Training with AWS Glue

Welcome to the PySpark Training repository. This repository contains materials and code for training on Apache Spark using AWS Glue.

## Table of Contents
- [Introduction](#introduction)
- [Prerequisites](#prerequisites)
- [Setup](#setup)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)
- [Trainer](#trainer)

## Introduction
This training is designed for individuals who are already familiar with Apache Spark at a medium to advanced level. You will learn how to process large datasets using PySpark and how to leverage AWS Glue for ETL (Extract, Transform, Load) operations.

## Topics Covered
In this training, we will cover the following topics:

  - Core Spark components
  - Spark's unified framework
  - RDDs
  - Lazy evaluation vs eager evaluation
  - Catalyst optimizer
  - Shuffling
  - Partitioning
  - Narrow vs Wide operations
  - Types of joins
  - Performance evaluation

## Prerequisites
Before you begin, ensure you have met the following requirements:
- An AWS account, or any other equivalent environment where you can run spark code.
- Medium to advanced knowledge of Python and Spark

## Setup
To set up the project, follow these steps:

1. Clone the repository:
    ```sh
    git clone https://github.com/your-username/pyspark-training.git
    cd pyspark-training
    ```

2. Upload the notebooks to your AWS Glue environment.

## Usage
To run the training notebooks, navigate to your AWS Glue console and start the notebooks.

## Trainer
The training is conducted by Oscar Mike Claure Cabrera. Connect with him on [LinkedIn](https://www.linkedin.com/in/oscarclaure/).
