# PySpark Training with AWS Glue

Welcome to the PySpark Training repository. This repository contains materials and code for training on Apache Spark using AWS Glue.

## Table of Contents
- [Introduction](#introduction)
- [Prerequisites](#prerequisites)
- [Setup](#setup)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)
- [Trainer](#trainer)

## Introduction
This training is designed to help you get started with Apache Spark and AWS Glue. You will learn how to process large datasets using PySpark and how to leverage AWS Glue for ETL (Extract, Transform, Load) operations.

## Topics Covered
In this training, we will cover the following topics:

  - Core Spark components
  - Spark's unified framework
  - RDDs
  - Lazy evaluation vs eager evaluation
  - Catalyst optimizer
  - Shuffling
  - Partitioning

## Prerequisites
Before you begin, ensure you have met the following requirements:
- An AWS account
- Basic knowledge of Python and Spark

## Setup
To set up the project, follow these steps:

1. Clone the repository:
    ```sh
    git clone https://github.com/your-username/pyspark-training.git
    cd pyspark-training
    ```

2. Upload the notebooks to your AWS Glue environment.

## Usage
To run the training notebooks, navigate to your AWS Glue console and start the notebooks.

## Contributing
Contributions are welcome! Please read the [contributing guidelines](CONTRIBUTING.md) for more information.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Trainer
The training is conducted by Oscar Mike Claure Cabrera. Connect with him on [LinkedIn](https://www.linkedin.com/in/oscarclaure/).
