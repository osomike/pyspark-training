{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%autosave 120",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"application/javascript": "IPython.notebook.set_autosave_interval(120000)"
					},
					"metadata": {}
				},
				{
					"name": "stdout",
					"text": "Autosaving every 120 seconds\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# PySpark Training Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "#%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 30\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 4",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 30 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 3\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--enable-spark-ui\": \"true\",\n    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n    \"--enable-metrics\": \"true\",\n    \"--enable-observability-metrics\": \"true\",\n    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n    \"--conf\": \"spark.sql.ui.retainedStages=100\",\n    \"--conf\": \"spark.sql.codegen.comments=true\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://dip-pyspark-training/spark_ui_tmp/', '--enable-metrics': 'true', '--enable-observability-metrics': 'true', '--conf': 'spark.sql.codegen.comments=true'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 3\nIdle Timeout: 30\nSession ID: 8c4a0016-6f30-452a-a956-637b30f693ce\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--enable-spark-ui true\n--spark-event-logs-path s3://dip-pyspark-training/spark_ui_tmp/\n--enable-metrics true\n--enable-observability-metrics true\n--conf spark.sql.codegen.comments=true\nWaiting for session 8c4a0016-6f30-452a-a956-637b30f693ce to get into ready status...\nSession 8c4a0016-6f30-452a-a956-637b30f693ce has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Get spark configuration",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\ndynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\ndynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\ndynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n\nexecutor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\nexecutor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\nexecutor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n\ndriver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\ndriver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n\nprint(f'''\nDynamic allocation enabled: {dynamic_allocation_enabled}\nDynamic min executors: {dynamic_min_executors}\nDynamic max executors: {dynamic_max_executors}\nDynamic initial executors: {dynamic_initial_executors}\n----------------------------------------\nExecutor instances: {executor_instances}\nExecutor cores: {executor_cores}\nExecutor memory: {executor_memory}\n----------------------------------------\nDriver cores: {driver_cores}\nDriver memory: {driver_memory}\n''')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\nDynamic allocation enabled: false\nDynamic min executors: 1\nDynamic max executors: 2\nDynamic initial executors: 3\n----------------------------------------\nExecutor instances: 2\nExecutor cores: 4\nExecutor memory: 10g\n----------------------------------------\nDriver cores: 4\nDriver memory: 10g\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Import libraries",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nimport datetime",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Load data NY Taxi dataset",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/big/ny-taxi-dataset/')\np_df = spark.read.format('parquet').load('s3://dip-pyspark-training//data/big/ny-taxi-dataset-partitioned/')\n#df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#p_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "df.schema",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n|      VTS|2009-11-07 19:44:00|2009-11-07 19:49:00|              2|         0.74|      -73.992127|      40.734658|        NULL|              NULL|        -73.99197|       40.729115|        CASH|        4.5|  0.0|    0.5|       0.0|         0.0|         5.0|\n|      VTS|2009-11-08 02:06:00|2009-11-08 02:12:00|              1|         1.04|      -74.008553|      40.719682|        NULL|              NULL|        -74.01615|       40.709963|        CASH|        5.7|  0.5|    0.5|       0.0|         0.0|         6.7|\n|      VTS|2009-11-08 03:57:00|2009-11-08 04:09:00|              1|         4.05|      -74.007742|      40.717097|        NULL|              NULL|       -73.986758|       40.768467|        CASH|       11.7|  0.5|    0.5|       0.0|         0.0|        12.7|\n|      VTS|2009-11-05 12:56:00|2009-11-05 12:58:00|              1|         0.33|      -73.992995|       40.71258|        NULL|              NULL|       -73.992995|        40.71258|        CASH|        3.3|  0.0|    0.5|       0.0|         0.0|         3.8|\n|      VTS|2009-11-05 08:35:00|2009-11-05 08:41:00|              1|          0.6|       -74.00921|      40.712933|        NULL|              NULL|       -74.002663|       40.716882|        CASH|        4.5|  0.0|    0.5|       0.0|         0.0|         5.0|\n|      VTS|2009-11-05 08:15:00|2009-11-05 08:24:00|              2|         1.37|       -74.00004|      40.719218|        NULL|              NULL|        -73.98615|        40.72989|      Credit|        6.5|  0.0|    0.5|       4.0|         0.0|        11.0|\n|      VTS|2009-11-08 13:48:00|2009-11-08 14:02:00|              1|         3.03|       -73.99134|      40.730082|        NULL|              NULL|       -73.966448|        40.76754|        CASH|       10.5|  0.0|    0.5|       0.0|         0.0|        11.0|\n|      VTS|2009-11-08 19:26:00|2009-11-08 19:38:00|              1|         2.32|      -73.997913|      40.716873|        NULL|              NULL|       -73.989532|       40.702203|        CASH|        8.5|  0.0|    0.5|       0.0|         0.0|         9.0|\n|      VTS|2009-11-05 22:25:00|2009-11-05 22:40:00|              1|         4.35|      -74.008048|      40.723218|        NULL|              NULL|       -73.941077|       40.717713|        CASH|       13.7|  0.5|    0.5|       0.0|         0.0|        14.7|\n|      VTS|2009-11-05 19:10:00|2009-11-05 19:18:00|              1|         1.63|      -74.000868|      40.733013|        NULL|              NULL|       -74.003182|       40.750095|        CASH|        6.9|  1.0|    0.5|       0.0|         0.0|         8.4|\n|      VTS|2009-11-08 18:12:00|2009-11-08 18:22:00|              1|         2.14|      -73.996468|      40.723705|        NULL|              NULL|       -73.978422|       40.750948|      Credit|        8.1|  0.0|    0.5|       2.0|         0.0|        10.6|\n|      VTS|2009-11-05 11:46:00|2009-11-05 12:07:00|              1|        11.34|      -74.009693|      40.703855|        NULL|              NULL|       -73.885265|         40.7709|      Credit|       26.1|  0.0|    0.5|       5.0|         0.0|        31.6|\n|      VTS|2009-11-08 23:31:00|2009-11-08 23:43:00|              1|         5.06|      -73.993887|      40.681733|        NULL|              NULL|       -73.978903|        40.71375|        CASH|       13.7|  0.5|    0.5|       0.0|         0.0|        14.7|\n|      VTS|2009-11-08 02:32:00|2009-11-08 02:40:00|              5|         1.16|      -74.004352|      40.730982|        NULL|              NULL|       -73.989567|       40.723233|      Credit|        6.5|  0.5|    0.5|       1.0|         0.0|         8.5|\n|      VTS|2009-11-05 13:45:00|2009-11-05 14:03:00|              1|         3.74|      -74.008378|      40.714382|        NULL|              NULL|       -73.975542|        40.75576|      Credit|       12.5|  0.0|    0.5|       2.0|         0.0|        15.0|\n|      VTS|2009-11-07 19:53:00|2009-11-07 20:05:00|              3|         2.05|      -74.009545|       40.71014|        NULL|              NULL|       -73.988218|       40.722012|        CASH|        8.1|  0.0|    0.5|       0.0|         0.0|         8.6|\n|      VTS|2009-11-05 19:21:00|2009-11-05 19:23:00|              5|         0.44|      -73.999923|      40.730373|        NULL|              NULL|       -74.003215|       40.732828|        CASH|        3.7|  1.0|    0.5|       0.0|         0.0|         5.2|\n|      VTS|2009-11-08 04:02:00|2009-11-08 04:09:00|              1|         1.48|      -73.991625|      40.722798|        NULL|              NULL|         -74.0064|       40.732785|        CASH|        6.9|  0.5|    0.5|       0.0|         0.0|         7.9|\n|      VTS|2009-11-05 08:22:00|2009-11-05 08:28:00|              2|         0.86|      -74.004705|      40.723917|        NULL|              NULL|       -74.007175|       40.733275|        CASH|        5.3|  0.0|    0.5|       0.0|         0.0|         5.8|\n|      VTS|2009-11-08 19:21:00|2009-11-08 19:42:00|              1|         4.19|      -74.006163|      40.733892|        NULL|              NULL|       -73.963665|       40.769308|        CASH|       14.1|  0.0|    0.5|       0.0|         0.0|        14.6|\n+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#df.filter(F.col('vendor_id') == 'VTS').explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "df2 = df.withColumn('surcharge_amount', F.col('total_amount') * 0.1)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df3 = df2.withColumn('is_long_trip', F.col('trip_distance') > 10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df4 = df3.withColumn('trip_category', F.when(F.col('passenger_count') <= 2, F.lit('small group')).when(F.col('passenger_count') <= 4, F.lit('medium group')).otherwise(F.lit('big group')))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df5 = df4.filter(F.col('vendor_id') == 'VTS')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df6 = df5.select(['vendor_id', 'total_amount', 'surcharge_amount', 'trip_distance', 'is_long_trip', 'passenger_count', 'trip_category'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df6.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "513853109\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df6.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Project ['vendor_id, 'total_amount, 'surcharge_amount, 'trip_distance, 'is_long_trip, 'passenger_count, 'trip_category]\n+- Filter (vendor_id#0 = VTS)\n   +- Project [vendor_id#0, pickup_datetime#1, dropoff_datetime#2, passenger_count#3, trip_distance#4, pickup_longitude#5, pickup_latitude#6, rate_code_id#7, store_and_fwd_flag#8, dropoff_longitude#9, dropoff_latitude#10, payment_type#11, fare_amount#12, extra#13, mta_tax#14, tip_amount#15, tolls_amount#16, total_amount#17, surcharge_amount#72, is_long_trip#93, CASE WHEN (passenger_count#3 <= 2) THEN small group WHEN (passenger_count#3 <= 4) THEN medium group ELSE big group END AS trip_category#114]\n      +- Project [vendor_id#0, pickup_datetime#1, dropoff_datetime#2, passenger_count#3, trip_distance#4, pickup_longitude#5, pickup_latitude#6, rate_code_id#7, store_and_fwd_flag#8, dropoff_longitude#9, dropoff_latitude#10, payment_type#11, fare_amount#12, extra#13, mta_tax#14, tip_amount#15, tolls_amount#16, total_amount#17, surcharge_amount#72, (trip_distance#4 > cast(10 as double)) AS is_long_trip#93]\n         +- Project [vendor_id#0, pickup_datetime#1, dropoff_datetime#2, passenger_count#3, trip_distance#4, pickup_longitude#5, pickup_latitude#6, rate_code_id#7, store_and_fwd_flag#8, dropoff_longitude#9, dropoff_latitude#10, payment_type#11, fare_amount#12, extra#13, mta_tax#14, tip_amount#15, tolls_amount#16, total_amount#17, (total_amount#17 * 0.1) AS surcharge_amount#72]\n            +- Relation [vendor_id#0,pickup_datetime#1,dropoff_datetime#2,passenger_count#3,trip_distance#4,pickup_longitude#5,pickup_latitude#6,rate_code_id#7,store_and_fwd_flag#8,dropoff_longitude#9,dropoff_latitude#10,payment_type#11,fare_amount#12,extra#13,mta_tax#14,tip_amount#15,tolls_amount#16,total_amount#17] parquet\n\n== Analyzed Logical Plan ==\nvendor_id: string, total_amount: double, surcharge_amount: double, trip_distance: double, is_long_trip: boolean, passenger_count: int, trip_category: string\nProject [vendor_id#0, total_amount#17, surcharge_amount#72, trip_distance#4, is_long_trip#93, passenger_count#3, trip_category#114]\n+- Filter (vendor_id#0 = VTS)\n   +- Project [vendor_id#0, pickup_datetime#1, dropoff_datetime#2, passenger_count#3, trip_distance#4, pickup_longitude#5, pickup_latitude#6, rate_code_id#7, store_and_fwd_flag#8, dropoff_longitude#9, dropoff_latitude#10, payment_type#11, fare_amount#12, extra#13, mta_tax#14, tip_amount#15, tolls_amount#16, total_amount#17, surcharge_amount#72, is_long_trip#93, CASE WHEN (passenger_count#3 <= 2) THEN small group WHEN (passenger_count#3 <= 4) THEN medium group ELSE big group END AS trip_category#114]\n      +- Project [vendor_id#0, pickup_datetime#1, dropoff_datetime#2, passenger_count#3, trip_distance#4, pickup_longitude#5, pickup_latitude#6, rate_code_id#7, store_and_fwd_flag#8, dropoff_longitude#9, dropoff_latitude#10, payment_type#11, fare_amount#12, extra#13, mta_tax#14, tip_amount#15, tolls_amount#16, total_amount#17, surcharge_amount#72, (trip_distance#4 > cast(10 as double)) AS is_long_trip#93]\n         +- Project [vendor_id#0, pickup_datetime#1, dropoff_datetime#2, passenger_count#3, trip_distance#4, pickup_longitude#5, pickup_latitude#6, rate_code_id#7, store_and_fwd_flag#8, dropoff_longitude#9, dropoff_latitude#10, payment_type#11, fare_amount#12, extra#13, mta_tax#14, tip_amount#15, tolls_amount#16, total_amount#17, (total_amount#17 * 0.1) AS surcharge_amount#72]\n            +- Relation [vendor_id#0,pickup_datetime#1,dropoff_datetime#2,passenger_count#3,trip_distance#4,pickup_longitude#5,pickup_latitude#6,rate_code_id#7,store_and_fwd_flag#8,dropoff_longitude#9,dropoff_latitude#10,payment_type#11,fare_amount#12,extra#13,mta_tax#14,tip_amount#15,tolls_amount#16,total_amount#17] parquet\n\n== Optimized Logical Plan ==\nProject [vendor_id#0, total_amount#17, (total_amount#17 * 0.1) AS surcharge_amount#72, trip_distance#4, (trip_distance#4 > 10.0) AS is_long_trip#93, passenger_count#3, CASE WHEN (passenger_count#3 <= 2) THEN small group WHEN (passenger_count#3 <= 4) THEN medium group ELSE big group END AS trip_category#114]\n+- Filter (isnotnull(vendor_id#0) AND (vendor_id#0 = VTS))\n   +- Relation [vendor_id#0,pickup_datetime#1,dropoff_datetime#2,passenger_count#3,trip_distance#4,pickup_longitude#5,pickup_latitude#6,rate_code_id#7,store_and_fwd_flag#8,dropoff_longitude#9,dropoff_latitude#10,payment_type#11,fare_amount#12,extra#13,mta_tax#14,tip_amount#15,tolls_amount#16,total_amount#17] parquet\n\n== Physical Plan ==\n*(1) Project [vendor_id#0, total_amount#17, (total_amount#17 * 0.1) AS surcharge_amount#72, trip_distance#4, (trip_distance#4 > 10.0) AS is_long_trip#93, passenger_count#3, CASE WHEN (passenger_count#3 <= 2) THEN small group WHEN (passenger_count#3 <= 4) THEN medium group ELSE big group END AS trip_category#114]\n+- *(1) Filter (isnotnull(vendor_id#0) AND (vendor_id#0 = VTS))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet [vendor_id#0,passenger_count#3,trip_distance#4,total_amount#17] Batched: true, DataFilters: [isnotnull(vendor_id#0), (vendor_id#0 = VTS)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/big/ny-taxi-dataset], PartitionFilters: [], PushedFilters: [IsNotNull(vendor_id), EqualTo(vendor_id,VTS)], ReadSchema: struct<vendor_id:string,passenger_count:int,trip_distance:double,total_amount:double>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df6.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------------+------------------+-------------+------------+---------------+-------------+\n|vendor_id|total_amount|  surcharge_amount|trip_distance|is_long_trip|passenger_count|trip_category|\n+---------+------------+------------------+-------------+------------+---------------+-------------+\n|      VTS|         5.0|               0.5|         0.74|       false|              2|  small group|\n|      VTS|         6.7|              0.67|         1.04|       false|              1|  small group|\n|      VTS|        12.7|              1.27|         4.05|       false|              1|  small group|\n|      VTS|         3.8|              0.38|         0.33|       false|              1|  small group|\n|      VTS|         5.0|               0.5|          0.6|       false|              1|  small group|\n|      VTS|        11.0|               1.1|         1.37|       false|              2|  small group|\n|      VTS|        11.0|               1.1|         3.03|       false|              1|  small group|\n|      VTS|         9.0|               0.9|         2.32|       false|              1|  small group|\n|      VTS|        14.7|              1.47|         4.35|       false|              1|  small group|\n|      VTS|         8.4|0.8400000000000001|         1.63|       false|              1|  small group|\n|      VTS|        10.6|              1.06|         2.14|       false|              1|  small group|\n|      VTS|        31.6|              3.16|        11.34|        true|              1|  small group|\n|      VTS|        14.7|              1.47|         5.06|       false|              1|  small group|\n|      VTS|         8.5|0.8500000000000001|         1.16|       false|              5|    big group|\n|      VTS|        15.0|               1.5|         3.74|       false|              1|  small group|\n|      VTS|         8.6|              0.86|         2.05|       false|              3| medium group|\n|      VTS|         5.2|              0.52|         0.44|       false|              5|    big group|\n|      VTS|         7.9|              0.79|         1.48|       false|              1|  small group|\n|      VTS|         5.8|              0.58|         0.86|       false|              2|  small group|\n|      VTS|        14.6|              1.46|         4.19|       false|              1|  small group|\n+---------+------------+------------------+-------------+------------+---------------+-------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ts = datetime.datetime.now()\noutput_file_path = 's3://dip-pyspark-training/output/dummy-output-01'\ndf6.write.mode('overwrite').format('parquet').save('s3://dip-pyspark-training/output/dummy-output-01')\npt = (datetime.datetime.now() - ts).seconds\nprint(f'The processing time was {pt} seconds')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "The processing time was 123 seconds\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "c_non_partitioned = spark.read.format('parquet').load(output_file_path).count()\nc_non_partitioned",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "513853109\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Using a partitioned source",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "p_df2 = p_df.withColumn('surcharge_amount', F.col('total_amount') * 0.1)\np_df3 = p_df2.withColumn('is_long_trip', F.col('trip_distance') > 10)\np_df4 = p_df3.withColumn('trip_category', F.when(F.col('passenger_count') <= 2, F.lit('small group')).when(F.col('passenger_count') <= 4, F.lit('medium group')).otherwise(F.lit('big group')))\np_df5 = p_df4.filter(F.col('vendor_id') == 'VTS')\np_df6 = p_df5.select(['vendor_id', 'total_amount', 'surcharge_amount', 'trip_distance', 'is_long_trip', 'passenger_count', 'trip_category'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "p_df6.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Project ['vendor_id, 'total_amount, 'surcharge_amount, 'trip_distance, 'is_long_trip, 'passenger_count, 'trip_category]\n+- Filter (vendor_id#53 = VTS)\n   +- Project [pickup_datetime#36, dropoff_datetime#37, passenger_count#38, trip_distance#39, pickup_longitude#40, pickup_latitude#41, rate_code_id#42, store_and_fwd_flag#43, dropoff_longitude#44, dropoff_latitude#45, payment_type#46, fare_amount#47, extra#48, mta_tax#49, tip_amount#50, tolls_amount#51, total_amount#52, vendor_id#53, surcharge_amount#378, is_long_trip#399, CASE WHEN (passenger_count#38 <= 2) THEN small group WHEN (passenger_count#38 <= 4) THEN medium group ELSE big group END AS trip_category#420]\n      +- Project [pickup_datetime#36, dropoff_datetime#37, passenger_count#38, trip_distance#39, pickup_longitude#40, pickup_latitude#41, rate_code_id#42, store_and_fwd_flag#43, dropoff_longitude#44, dropoff_latitude#45, payment_type#46, fare_amount#47, extra#48, mta_tax#49, tip_amount#50, tolls_amount#51, total_amount#52, vendor_id#53, surcharge_amount#378, (trip_distance#39 > cast(10 as double)) AS is_long_trip#399]\n         +- Project [pickup_datetime#36, dropoff_datetime#37, passenger_count#38, trip_distance#39, pickup_longitude#40, pickup_latitude#41, rate_code_id#42, store_and_fwd_flag#43, dropoff_longitude#44, dropoff_latitude#45, payment_type#46, fare_amount#47, extra#48, mta_tax#49, tip_amount#50, tolls_amount#51, total_amount#52, vendor_id#53, (total_amount#52 * 0.1) AS surcharge_amount#378]\n            +- Relation [pickup_datetime#36,dropoff_datetime#37,passenger_count#38,trip_distance#39,pickup_longitude#40,pickup_latitude#41,rate_code_id#42,store_and_fwd_flag#43,dropoff_longitude#44,dropoff_latitude#45,payment_type#46,fare_amount#47,extra#48,mta_tax#49,tip_amount#50,tolls_amount#51,total_amount#52,vendor_id#53] parquet\n\n== Analyzed Logical Plan ==\nvendor_id: string, total_amount: double, surcharge_amount: double, trip_distance: double, is_long_trip: boolean, passenger_count: int, trip_category: string\nProject [vendor_id#53, total_amount#52, surcharge_amount#378, trip_distance#39, is_long_trip#399, passenger_count#38, trip_category#420]\n+- Filter (vendor_id#53 = VTS)\n   +- Project [pickup_datetime#36, dropoff_datetime#37, passenger_count#38, trip_distance#39, pickup_longitude#40, pickup_latitude#41, rate_code_id#42, store_and_fwd_flag#43, dropoff_longitude#44, dropoff_latitude#45, payment_type#46, fare_amount#47, extra#48, mta_tax#49, tip_amount#50, tolls_amount#51, total_amount#52, vendor_id#53, surcharge_amount#378, is_long_trip#399, CASE WHEN (passenger_count#38 <= 2) THEN small group WHEN (passenger_count#38 <= 4) THEN medium group ELSE big group END AS trip_category#420]\n      +- Project [pickup_datetime#36, dropoff_datetime#37, passenger_count#38, trip_distance#39, pickup_longitude#40, pickup_latitude#41, rate_code_id#42, store_and_fwd_flag#43, dropoff_longitude#44, dropoff_latitude#45, payment_type#46, fare_amount#47, extra#48, mta_tax#49, tip_amount#50, tolls_amount#51, total_amount#52, vendor_id#53, surcharge_amount#378, (trip_distance#39 > cast(10 as double)) AS is_long_trip#399]\n         +- Project [pickup_datetime#36, dropoff_datetime#37, passenger_count#38, trip_distance#39, pickup_longitude#40, pickup_latitude#41, rate_code_id#42, store_and_fwd_flag#43, dropoff_longitude#44, dropoff_latitude#45, payment_type#46, fare_amount#47, extra#48, mta_tax#49, tip_amount#50, tolls_amount#51, total_amount#52, vendor_id#53, (total_amount#52 * 0.1) AS surcharge_amount#378]\n            +- Relation [pickup_datetime#36,dropoff_datetime#37,passenger_count#38,trip_distance#39,pickup_longitude#40,pickup_latitude#41,rate_code_id#42,store_and_fwd_flag#43,dropoff_longitude#44,dropoff_latitude#45,payment_type#46,fare_amount#47,extra#48,mta_tax#49,tip_amount#50,tolls_amount#51,total_amount#52,vendor_id#53] parquet\n\n== Optimized Logical Plan ==\nProject [vendor_id#53, total_amount#52, (total_amount#52 * 0.1) AS surcharge_amount#378, trip_distance#39, (trip_distance#39 > 10.0) AS is_long_trip#399, passenger_count#38, CASE WHEN (passenger_count#38 <= 2) THEN small group WHEN (passenger_count#38 <= 4) THEN medium group ELSE big group END AS trip_category#420]\n+- Filter (isnotnull(vendor_id#53) AND (vendor_id#53 = VTS))\n   +- Relation [pickup_datetime#36,dropoff_datetime#37,passenger_count#38,trip_distance#39,pickup_longitude#40,pickup_latitude#41,rate_code_id#42,store_and_fwd_flag#43,dropoff_longitude#44,dropoff_latitude#45,payment_type#46,fare_amount#47,extra#48,mta_tax#49,tip_amount#50,tolls_amount#51,total_amount#52,vendor_id#53] parquet\n\n== Physical Plan ==\n*(1) Project [vendor_id#53, total_amount#52, (total_amount#52 * 0.1) AS surcharge_amount#378, trip_distance#39, (trip_distance#39 > 10.0) AS is_long_trip#399, passenger_count#38, CASE WHEN (passenger_count#38 <= 2) THEN small group WHEN (passenger_count#38 <= 4) THEN medium group ELSE big group END AS trip_category#420]\n+- *(1) ColumnarToRow\n   +- FileScan parquet [passenger_count#38,trip_distance#39,total_amount#52,vendor_id#53] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/big/ny-taxi-dataset-partitioned], PartitionFilters: [isnotnull(vendor_id#53), (vendor_id#53 = VTS)], PushedFilters: [], ReadSchema: struct<passenger_count:int,trip_distance:double,total_amount:double>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ts = datetime.datetime.now()\noutput_file_path = 's3://dip-pyspark-training/output/dummy-output-02'\np_df6.write.mode('overwrite').format('parquet').save(output_file_path)\np_pt = (datetime.datetime.now() - ts).seconds\nprint(f'The processing time was {p_pt} seconds')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "The processing time was 102 seconds\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "c_partitioned = spark.read.format('parquet').load(output_file_path).count()\nc_partitioned",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "513853109\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "assert c_non_partitioned == c_partitioned",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "d = round(100 * (pt - p_pt)/p_pt, 2)\nprint(f'Using the partitioned source was {d}% faster than the non-partitioned one.')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "Using the partitioned source was 20.59% faster than the non-partitioned one.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "1611611035\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "p_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "1611611035\n",
					"output_type": "stream"
				}
			]
		}
	]
}