{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# PySpark Training Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "\n####  Run this cell to set up and start your interactive session.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--enable-continuous-cloudwatch-log\": \"true\",\n    \"--enable-spark-ui\": \"true\",\n    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n    \"--enable-metrics\": \"true\",\n    \"--enable-observability-metrics\": \"true\",\n    \"--conf\": \"spark.sql.codegen.comments=true\",\n    \"--conf\": \"spark.sql.codegen.fallback=true\",\n    \"--conf\": \"spark.sql.codegen.wholeStage=true\",\n    \"--conf\": \"spark.sql.ui.explainMode=extended\",\n    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n    \"--conf\": \"spark.ui.retainedJobs=1000\",\n    \"--conf\": \"spark.ui.retainedStages=1000\",\n    \"--conf\": \"spark.ui.retainedTasks=10000\",\n    \"--conf\": \"spark.ui.showAdditionalMetrics=true\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--enable-continuous-cloudwatch-log': 'true', '--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://dip-pyspark-training/spark_ui_tmp/', '--enable-metrics': 'true', '--enable-observability-metrics': 'true', '--conf': 'spark.ui.showAdditionalMetrics=true'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 60\nSession ID: 613a7f7e-0199-479e-9501-18fe0a567a11\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--enable-continuous-cloudwatch-log true\n--enable-spark-ui true\n--spark-event-logs-path s3://dip-pyspark-training/spark_ui_tmp/\n--enable-metrics true\n--enable-observability-metrics true\n--conf spark.ui.showAdditionalMetrics=true\nWaiting for session 613a7f7e-0199-479e-9501-18fe0a567a11 to get into ready status...\nSession 613a7f7e-0199-479e-9501-18fe0a567a11 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Get spark's configuration",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\ndynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\ndynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\ndynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n\nexecutor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\nexecutor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\nexecutor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n\ndriver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\ndriver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n\nprint(f'''\nDynamic allocation enabled: {dynamic_allocation_enabled}\nDynamic min executors: {dynamic_min_executors}\nDynamic max executors: {dynamic_max_executors}\nDynamic initial executors: {dynamic_initial_executors}\n----------------------------------------\nExecutor instances: {executor_instances}\nExecutor cores: {executor_cores}\nExecutor memory: {executor_memory}\n----------------------------------------\nDriver cores: {driver_cores}\nDriver memory: {driver_memory}\n''')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\nDynamic allocation enabled: false\nDynamic min executors: 1\nDynamic max executors: 1\nDynamic initial executors: 3\n----------------------------------------\nExecutor instances: 1\nExecutor cores: 4\nExecutor memory: 10g\n----------------------------------------\nDriver cores: 4\nDriver memory: 10g\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sparkContext.getConf().getAll()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "[('spark.blacklist.decommissioning.enabled', 'true'), ('spark.metrics.conf.*.source.jvm.class', 'org.apache.spark.metrics.source.JvmSource'), ('spark.executor.extraJavaOptions', \"-Djava.net.preferIPv6Addresses=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.driver.cores', '4'), ('spark.hadoop.fs.AbstractFileSystem.s3.impl', 'org.apache.hadoop.fs.s3.EMRFSDelegate'), ('spark.sql.shuffle.partitions', '4'), ('spark.metrics.conf.driver.source.throughput.class', 'org.apache.spark.metrics.source.ThroughputMetricsSource'), ('spark.hadoop.fs.s3bfs.impl', 'org.apache.hadoop.fs.s3.S3FileSystem'), ('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'), ('spark.glue.glue-python-libs-dir', '/tmp/glue-job-7289043572118488777/python'), ('spark.sql.extensions', 'com.amazonaws.services.glue.spark.extensions.SparkGlueBookmarkExtension'), ('spark.yarn.dist.archives', 'file:///tmp/glue-job-7289043572118488777_glue_venv.zip#python_environment'), ('spark.sql.hive.metastore.sharedPrefixes', 'software.amazon.awssdk.services.dynamodb'), ('spark.app.id', 'spark-application-1736438093135'), ('spark.emr-serverless.client.describe.batch.size', '100'), ('spark.hadoop.fs.s3n.impl', 'com.amazon.ws.emr.hadoop.fs.EmrFileSystem'), ('spark.driver.host', '172.35.207.114'), ('spark.metrics.conf.*.sink.GlueCloudwatch.glueVersion', '5.0'), ('spark.metrics.conf.*.sink.GlueCloudwatch.jobName', '9cdb26e2-93a2-4578-940b-52c944395ddd'), ('spark.hadoop.fs.s3.impl', 'com.amazon.ws.emr.hadoop.fs.EmrFileSystem'), ('spark.pyspark.python', '/usr/bin/python3.11'), ('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.metrics.conf.*.sink.GlueCloudwatch.jobRunId', '613a7f7e-0199-479e-9501-18fe0a567a11'), ('spark.glue.enable-job-insights', 'false'), ('spark.app.initial.file.urls', 'spark://172.35.207.114:43457/files/hive-site.xml'), ('spark.metrics.conf.driver.source.glue.jobPerformance.class', 'org.apache.spark.metrics.source.PerformanceMetricsSource'), ('spark.master', 'custom:jes'), ('spark.app.initial.archive.urls', 'spark://172.35.207.114:43457/files/glue-job-7289043572118488777_glue_venv.zip#python_environment'), ('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true'), ('spark.decommissioning.timeout.threshold', '20'), ('spark.files', '/etc/spark/conf.dist/hive-site.xml'), ('spark.ui.enabled', 'false'), ('spark.default.parallelism', '4'), ('spark.repl.class.outputDir', '/tmp/spark16043457570807554087'), ('spark.glue.SESSION_ID', '613a7f7e-0199-479e-9501-18fe0a567a11'), ('spark.hadoop.fs.s3a.committer.magic.enabled', 'true'), ('spark.glue.additionalParams.USER_JARS_FIRST', 'false'), ('spark.metrics.conf.*.source.glue.verticalScaling.class', 'org.apache.spark.metrics.source.VerticalScalingMetricsSource'), ('spark.driver.extraClassPath', '/usr/lib/livy/rsc-jars/*:/usr/lib/livy/repl_2.12-jars/*:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/goodies/lib/emr-serverless-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar'), ('spark.metrics.conf.*.sink.GlueCloudwatch.legacyMetrics', 'true'), ('spark.submit.customResourceManager.submit.class', 'org.apache.spark.deploy.emrserverless.submit.EmrServerlessClientApplication'), ('spark.executor.cores', '4'), ('spark.glue.additionalParams.GLUE_LIBS_TEMP_DIR_PATH', '/tmp/glue-job-7289043572118488777/'), ('spark.hadoop.fs.s3a.committer.name', 'magicv2'), ('spark.dynamicAllocation.initialExecutors', '3'), ('spark.driver.port', '43457'), ('spark.glue.enable-observability-metrics', 'true'), ('spark.submit.deployMode', 'client'), ('spark.ui.custom.executor.log.url', '/logs/{{CONTAINER_ID}}/{{FILE_NAME}}.gz'), ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'), ('spark.glue.additionalParams.ADDITIONAL_CLASSPATH', '/tmp/glue-job-7289043572118488777/*:/usr/lib/livy/rsc-jars/*:/usr/lib/livy/repl_2.12-jars/*:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/goodies/lib/emr-serverless-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar:/usr/lib/spark/jars/*:null/jars/*::/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/hadoop/*:/tmp/glue-job-7289043572118488777/jars/*:/tmp/glue-job-7289043572118488777/python/*:/tmp/glue-job-7289043572118488777/aws_glue_connectors/marketplace/*:/usr/share/aws/aws-glue-shuffle/AWSGlueShuffle-5.0.0.jar:/usr/share/aws/emr-glue-bootstrap/EMRGlueBootstrap-5.0-jar-with-dependencies.jar:/usr/lib/spark/jars/HikariCP-2.5.1.jar:/usr/lib/spark/jars/JLargeArrays-1.5.jar:/usr/lib/spark/jars/JTransforms-3.1.jar:/usr/lib/spark/jars/RoaringBitmap-0.9.45.jar:/usr/lib/spark/jars/ST4-4.0.4.jar:/usr/lib/spark/jars/activation-1.1.1.jar:/usr/lib/spark/jars/aggdesigner-algorithm-6.0.jar:/usr/lib/spark/jars/aircompressor-0.27.jar:/usr/lib/spark/jars/algebra_2.12-2.0.1.jar:/usr/lib/spark/jars/animal-sniffer-annotations-1.23.jar:/usr/lib/spark/jars/annotations-17.0.0.jar:/usr/lib/spark/jars/annotations-4.1.1.4.jar:/usr/lib/spark/jars/antlr-runtime-3.5.2.jar:/usr/lib/spark/jars/antlr4-runtime-4.9.3.jar:/usr/lib/spark/jars/aopalliance-repackaged-2.6.1.jar:/usr/lib/spark/jars/arpack-3.0.3.jar:/usr/lib/spark/jars/arpack_combined_all-0.1.jar:/usr/lib/spark/jars/arrow-format-12.0.1.jar:/usr/lib/spark/jars/arrow-memory-core-12.0.1.jar:/usr/lib/spark/jars/arrow-memory-netty-12.0.1.jar:/usr/lib/spark/jars/arrow-vector-12.0.1.jar:/usr/lib/spark/jars/audience-annotations-0.12.0.jar:/usr/lib/spark/jars/avro-1.11.2.jar:/usr/lib/spark/jars/avro-ipc-1.11.2.jar:/usr/lib/spark/jars/avro-mapred-1.11.2.jar:/usr/lib/spark/jars/blas-3.0.3.jar:/usr/lib/spark/jars/bonecp-0.8.0.RELEASE.jar:/usr/lib/spark/jars/breeze-macros_2.12-2.1.0.jar:/usr/lib/spark/jars/breeze_2.12-2.1.0.jar:/usr/lib/spark/jars/cats-kernel_2.12-2.1.1.jar:/usr/lib/spark/jars/chill-java-0.10.0.jar:/usr/lib/spark/jars/chill_2.12-0.10.0.jar:/usr/lib/spark/jars/commons-cli-1.5.0.jar:/usr/lib/spark/jars/commons-codec-1.16.1.jar:/usr/lib/spark/jars/commons-collections-3.2.2.jar:/usr/lib/spark/jars/commons-collections4-4.4.jar:/usr/lib/spark/jars/commons-compiler-3.1.9.jar:/usr/lib/spark/jars/commons-compress-1.23.0.jar:/usr/lib/spark/jars/commons-crypto-1.1.0.jar:/usr/lib/spark/jars/commons-dbcp-1.4.jar:/usr/lib/spark/jars/commons-io-2.16.1.jar:/usr/lib/spark/jars/commons-lang-2.6.jar:/usr/lib/spark/jars/commons-lang3-3.12.0.jar:/usr/lib/spark/jars/commons-logging-1.1.3.jar:/usr/lib/spark/jars/commons-math3-3.6.1.jar:/usr/lib/spark/jars/commons-pool-1.5.4.jar:/usr/lib/spark/jars/commons-text-1.10.0.jar:/usr/lib/spark/jars/compress-lzf-1.1.2.jar:/usr/lib/spark/jars/curator-client-2.13.0.jar:/usr/lib/spark/jars/curator-framework-2.13.0.jar:/usr/lib/spark/jars/curator-recipes-2.13.0.jar:/usr/lib/spark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/lib/spark/jars/datanucleus-core-4.1.17.jar:/usr/lib/spark/jars/datanucleus-rdbms-4.1.19.jar:/usr/lib/spark/jars/datasketches-java-3.3.0.jar:/usr/lib/spark/jars/datasketches-memory-2.1.0.jar:/usr/lib/spark/jars/derby-10.14.2.0.jar:/usr/lib/spark/jars/disruptor-3.3.7.jar:/usr/lib/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/lib/spark/jars/emr-serverless-goodies-common.jar:/usr/lib/spark/jars/emr-serverless-spark-goodies.jar:/usr/lib/spark/jars/emr-spark-goodies.jar:/usr/lib/spark/jars/error_prone_annotations-2.18.0.jar:/usr/lib/spark/jars/flatbuffers-java-1.12.0.jar:/usr/lib/spark/jars/glue-spark-goodies-2.15.0.jar:/usr/lib/spark/jars/gmetric4j-1.0.10.jar:/usr/lib/spark/jars/grpc-api-1.56.0.jar:/usr/lib/spark/jars/grpc-context-1.56.0.jar:/usr/lib/spark/jars/grpc-core-1.56.0.jar:/usr/lib/spark/jars/grpc-netty-1.56.0.jar:/usr/lib/spark/jars/grpc-protobuf-1.56.0.jar:/usr/lib/spark/jars/grpc-protobuf-lite-1.56.0.jar:/usr/lib/spark/jars/grpc-services-1.56.0.jar:/usr/lib/spark/jars/grpc-stub-1.56.0.jar:/usr/lib/spark/jars/gson-2.10.1.jar:/usr/lib/spark/jars/guava-14.0.1.jar:/usr/lib/spark/jars/hadoop-client-api-3.4.0-amzn-1.jar:/usr/lib/spark/jars/hadoop-client-runtime-3.4.0-amzn-1.jar:/usr/lib/spark/jars/hadoop-shaded-guava-1.2.0.jar:/usr/lib/spark/jars/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar:/usr/lib/spark/jars/hive-beeline-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-cli-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-common-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-exec-2.3.9-amzn-4-core.jar:/usr/lib/spark/jars/hive-jdbc-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-llap-common-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-metastore-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-serde-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-service-rpc-3.1.3.jar:/usr/lib/spark/jars/hive-shims-0.23-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-shims-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-shims-common-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-shims-scheduler-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-storage-api-2.8.1.jar:/usr/lib/spark/jars/hk2-api-2.6.1.jar:/usr/lib/spark/jars/hk2-locator-2.6.1.jar:/usr/lib/spark/jars/hk2-utils-2.6.1.jar:/usr/lib/spark/jars/httpcore-4.4.13.jar:/usr/lib/spark/jars/httpclient-4.5.13.jar:/usr/lib/spark/jars/istack-commons-runtime-3.0.8.jar:/usr/lib/spark/jars/ivy-2.5.1.jar:/usr/lib/spark/jars/jackson-annotations-2.15.2.jar:/usr/lib/spark/jars/jackson-core-2.15.2.jar:/usr/lib/spark/jars/jackson-core-asl-1.9.13.jar:/usr/lib/spark/jars/jackson-databind-2.15.2.jar:/usr/lib/spark/jars/jackson-dataformat-yaml-2.15.2.jar:/usr/lib/spark/jars/jackson-datatype-jsr310-2.15.2.jar:/usr/lib/spark/jars/jackson-mapper-asl-1.9.13.jar:/usr/lib/spark/jars/jackson-module-scala_2.12-2.15.2.jar:/usr/lib/spark/jars/jakarta.annotation-api-1.3.5.jar:/usr/lib/spark/jars/jakarta.inject-2.6.1.jar:/usr/lib/spark/jars/jakarta.servlet-api-4.0.3.jar:/usr/lib/spark/jars/jakarta.validation-api-2.0.2.jar:/usr/lib/spark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/lib/spark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/spark/jars/janino-3.1.9.jar:/usr/lib/spark/jars/javassist-3.29.2-GA.jar:/usr/lib/spark/jars/javax.jdo-3.2.0-m3.jar:/usr/lib/spark/jars/javax.servlet-api-3.1.0.jar:/usr/lib/spark/jars/javolution-5.5.1.jar:/usr/lib/spark/jars/jaxb-runtime-2.3.2.jar:/usr/lib/spark/jars/jcl-over-slf4j-2.0.7.jar:/usr/lib/spark/jars/jdo-api-3.0.1.jar:/usr/lib/spark/jars/jersey-client-2.40.jar:/usr/lib/spark/jars/jersey-common-2.40.jar:/usr/lib/spark/jars/jersey-container-servlet-2.40.jar:/usr/lib/spark/jars/jersey-container-servlet-core-2.40.jar:/usr/lib/spark/jars/jersey-hk2-2.40.jar:/usr/lib/spark/jars/jersey-server-2.40.jar:/usr/lib/spark/jars/jetty-rewrite-9.3.27.v20190418.jar:/usr/lib/spark/jars/jline-2.14.6.jar:/usr/lib/spark/jars/joda-time-2.12.5.jar:/usr/lib/spark/jars/jodd-core-3.5.2.jar:/usr/lib/spark/jars/jpam-1.1.jar:/usr/lib/spark/jars/json-1.8.jar:/usr/lib/spark/jars/json4s-ast_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/json4s-core_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/jsr305-3.0.0.jar:/usr/lib/spark/jars/jta-1.1.jar:/usr/lib/spark/jars/jul-to-slf4j-2.0.7.jar:/usr/lib/spark/jars/kryo-shaded-4.0.2.jar:/usr/lib/spark/jars/lapack-3.0.3.jar:/usr/lib/spark/jars/leveldbjni-all-1.8.jar:/usr/lib/spark/jars/libfb303-0.9.3.jar:/usr/lib/spark/jars/libthrift-0.12.0.jar:/usr/lib/spark/jars/log4j-1.2-api-2.20.0.jar:/usr/lib/spark/jars/log4j-api-2.20.0.jar:/usr/lib/spark/jars/log4j-core-2.20.0.jar:/usr/lib/spark/jars/log4j-slf4j2-impl-2.20.0.jar:/usr/lib/spark/jars/logging-interceptor-3.12.12.jar:/usr/lib/spark/jars/lz4-java-1.8.0.jar:/usr/lib/spark/jars/metrics-core-4.2.19.jar:/usr/lib/spark/jars/metrics-graphite-4.2.19.jar:/usr/lib/spark/jars/metrics-jmx-4.2.19.jar:/usr/lib/spark/jars/metrics-json-4.2.19.jar:/usr/lib/spark/jars/metrics-jvm-4.2.19.jar:/usr/lib/spark/jars/minlog-1.3.0.jar:/usr/lib/spark/jars/netty-all-4.1.100.Final.jar:/usr/lib/spark/jars/netty-buffer-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-http-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-http2-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-socks-4.1.100.Final.jar:/usr/lib/spark/jars/netty-common-4.1.100.Final.jar:/usr/lib/spark/jars/netty-handler-4.1.100.Final.jar:/usr/lib/spark/jars/netty-handler-proxy-4.1.100.Final.jar:/usr/lib/spark/jars/netty-resolver-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/lib/spark/jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/lib/spark/jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/lib/spark/jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/lib/spark/jars/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/lib/spark/jars/objenesis-3.3.jar:/usr/lib/spark/jars/okhttp-3.12.12.jar:/usr/lib/spark/jars/okio-1.15.0.jar:/usr/lib/spark/jars/opencsv-2.3.jar:/usr/lib/spark/jars/orc-core-1.9.4-shaded-protobuf.jar:/usr/lib/spark/jars/orc-mapreduce-1.9.4-shaded-protobuf.jar:/usr/lib/spark/jars/orc-shims-1.9.4.jar:/usr/lib/spark/jars/oro-2.0.8.jar:/usr/lib/spark/jars/osgi-resource-locator-1.0.3.jar:/usr/lib/spark/jars/paranamer-2.8.jar:/usr/lib/spark/jars/parquet-column-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-common-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-encoding-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-format-structures-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-hadoop-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-jackson-1.13.1-amzn-3.jar:/usr/lib/spark/jars/perfmark-api-0.26.0.jar:/usr/lib/spark/jars/pickle-1.3.jar:/usr/lib/spark/jars/proto-google-common-protos-2.17.0.jar:/usr/lib/spark/jars/py4j-0.10.9.7.jar:/usr/lib/spark/jars/remotetea-oncrpc-1.1.2.jar:/usr/lib/spark/jars/rocksdbjni-8.3.2.jar:/usr/lib/spark/jars/scala-collection-compat_2.12-2.7.0.jar:/usr/lib/spark/jars/scala-compiler-2.12.18.jar:/usr/lib/spark/jars/scala-library-2.12.18.jar:/usr/lib/spark/jars/scala-parser-combinators_2.12-2.3.0.jar:/usr/lib/spark/jars/scala-reflect-2.12.18.jar:/usr/lib/spark/jars/scala-xml_2.12-2.1.0.jar:/usr/lib/spark/jars/shims-0.9.45.jar:/usr/lib/spark/jars/slf4j-api-2.0.7.jar:/usr/lib/spark/jars/snakeyaml-2.1.jar:/usr/lib/spark/jars/snakeyaml-engine-2.6.jar:/usr/lib/spark/jars/snappy-java-1.1.10.5.jar:/usr/lib/spark/jars/spark-acl-1.0.0.jar:/usr/lib/spark/jars/spark-catalyst_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-common-utils_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-core_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-fgac_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-graphx_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-hive_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-kvstore_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-launcher_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-mllib-local_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-mllib_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-network-common_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-network-shuffle_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-repl_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-sketch_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-sql-api_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-sql_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-streaming_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-tags_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-unsafe_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-yarn_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spire-macros_2.12-0.17.0.jar:/usr/lib/spark/jars/spire-platform_2.12-0.17.0.jar:/usr/lib/spark/jars/spire-util_2.12-0.17.0.jar:/usr/lib/spark/jars/spire_2.12-0.17.0.jar:/usr/lib/spark/jars/stax-api-1.0.1.jar:/usr/lib/spark/jars/stream-2.9.6.jar:/usr/lib/spark/jars/super-csv-2.2.0.jar:/usr/lib/spark/jars/threeten-extra-1.7.1.jar:/usr/lib/spark/jars/tink-1.9.0.jar:/usr/lib/spark/jars/transaction-api-1.1.jar:/usr/lib/spark/jars/univocity-parsers-2.9.1.jar:/usr/lib/spark/jars/volcano-client-6.7.2.jar:/usr/lib/spark/jars/volcano-model-v1beta1-6.7.2.jar:/usr/lib/spark/jars/xbean-asm9-shaded-4.23.jar:/usr/lib/spark/jars/xz-1.9.jar:/usr/lib/spark/jars/zjsonpatch-0.3.0.jar:/usr/lib/spark/jars/zookeeper-3.9.1.jar:/usr/lib/spark/jars/zookeeper-jute-3.9.1.jar:/usr/lib/spark/jars/zstd-jni-1.5.5-4.jar:/usr/lib/spark/conf:/usr/share/aws/aws-java-sdk-v2/aws-sdk-java-bundle-2.28.8.jar:/usr/share/aws/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-client-common-4.2.0.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-hive3-client-4.2.0.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-hive3-client.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client-4.2.0.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-flink-runtime-1.19-1.6.1-amzn-1.jar:/usr/share/aws/iceberg/lib/iceberg-flink-runtime.jar:/usr/share/aws/iceberg/lib/iceberg-hive-runtime-1.6.1-amzn-1.jar:/usr/share/aws/iceberg/lib/iceberg-hive3-runtime.jar:/usr/share/aws/iceberg/lib/iceberg-spark-runtime-3.5_2.12-1.6.1-amzn-1.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar:/usr/share/aws/redshift/jdbc/AwsSecretsManagerCachingJava.jar:/usr/share/aws/redshift/jdbc/AwsSecretsManagerJDBC.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC42.jar:/usr/share/aws/redshift/jdbc/aws-secretsmanager-caching-java-1.0.2.jar:/usr/share/aws/redshift/jdbc/aws-secretsmanager-jdbc-1.0.12.jar:/usr/share/aws/redshift/jdbc/redshift-jdbc42-2.1.0.29.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-avro.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-avro_2.12-3.5.2-amzn-1.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-redshift.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-redshift_2.12-6.3.0-spark_3.5.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-AzureCosmos-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-AzureSQL-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-BigQuery-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-MongoDB-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-MySQL-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-OpenSearch-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Oracle-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-PostgreSQL-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-SAPHana-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-SQLServer-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Snowflake-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Teradata-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Vertica-1.0.jar:/usr/share/aws/glue-pii-dependencies/lib/stanford-corenlp-3.6.0-models.jar:/usr/share/aws/datazone-openlineage-spark/lib/DataZoneOpenLineageSpark-1.0.jar:/lib/*::/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.772.jar:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/AssumeRoleAWSCredentialsProvider-1.1-SNAPSHOT.jar:/usr/share/aws/emr/emrfs/lib/animal-sniffer-annotations-1.14.jar:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/checker-qual-2.5.2.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.66.0.jar:/usr/share/aws/emr/emrfs/lib/error_prone_annotations-2.1.3.jar:/usr/share/aws/emr/emrfs/lib/findbugs-annotations-3.0.1.jar:/usr/share/aws/emr/emrfs/lib/j2objc-annotations-1.1.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.12.705.jar:/usr/share/aws/emr/emrfs/lib/jsr305-3.0.2.jar:/usr/share/aws/emr/emrfs/lib/kryo-shaded-4.0.2.jar:/usr/share/aws/emr/emrfs/lib/minlog-1.3.0.jar:/usr/share/aws/emr/emrfs/lib/objenesis-2.5.1.jar:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/hadoop/hadoop-aliyun-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-aliyun.jar:/usr/lib/hadoop/hadoop-annotations-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-annotations.jar:/usr/lib/hadoop/hadoop-archive-logs-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-archive-logs.jar:/usr/lib/hadoop/hadoop-archives-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-archives.jar:/usr/lib/hadoop/hadoop-auth-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-auth.jar:/usr/lib/hadoop/hadoop-aws-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-aws.jar:/usr/lib/hadoop/hadoop-azure-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-azure-datalake-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-azure-datalake.jar:/usr/lib/hadoop/hadoop-azure.jar:/usr/lib/hadoop/hadoop-client-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-client.jar:/usr/lib/hadoop/hadoop-common-3.4.0-amzn-1-tests.jar:/usr/lib/hadoop/hadoop-common-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-common.jar:/usr/lib/hadoop/hadoop-datajoin-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-datajoin.jar:/usr/lib/hadoop/hadoop-distcp-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-distcp.jar:/usr/lib/hadoop/hadoop-dynamometer-blockgen-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-dynamometer-blockgen.jar:/usr/lib/hadoop/hadoop-dynamometer-infra-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-dynamometer-infra.jar:/usr/lib/hadoop/hadoop-dynamometer-workload-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-dynamometer-workload.jar:/usr/lib/hadoop/hadoop-extras-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-extras.jar:/usr/lib/hadoop/hadoop-federation-balance-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-federation-balance.jar:/usr/lib/hadoop/hadoop-fs2img-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-fs2img.jar:/usr/lib/hadoop/hadoop-gridmix-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-gridmix.jar:/usr/lib/hadoop/hadoop-kafka-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-kafka.jar:/usr/lib/hadoop/hadoop-kms-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-kms.jar:/usr/lib/hadoop/hadoop-minicluster-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-minicluster.jar:/usr/lib/hadoop/hadoop-nfs-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-nfs.jar:/usr/lib/hadoop/hadoop-registry-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-registry.jar:/usr/lib/hadoop/hadoop-resourceestimator-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-resourceestimator.jar:/usr/lib/hadoop/hadoop-rumen-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-rumen.jar:/usr/lib/hadoop/hadoop-shaded-guava-1.2.0.jar:/usr/lib/hadoop/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/lib/hadoop/hadoop-sls-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-sls.jar:/usr/lib/hadoop/hadoop-streaming-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-streaming.jar:/etc/hadoop/conf:/usr/share/java/Hive-JSON-Serde/hive-openx-serde-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/java/Hive-JSON-Serde/json-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/json-serde-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/json-serde.jar:/usr/share/java/Hive-JSON-Serde/json-udf-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/json-udf.jar:/usr/share/java/Hive-JSON-Serde/json.jar:/tmp/glue-job-7289043572118488777/extra-jars/*'), ('spark.metrics.conf.*.sink.GlueCloudwatch.proxyPort', '-1'), ('spark.metrics.conf.*.sink.GlueCloudwatch.class', 'org.apache.spark.metrics.sink.GlueCloudwatchSink'), ('spark.glue.GLUE_VERSION', '5.0'), ('spark.jars', '\"/tmp/folder-does-not-exists/\",file:///tmp/glue-job-7289043572118488777/jars/liLJjV-aws-glue-di-package-5.0.296.jar,/tmp/glue-job-7289043572118488777/jars/b5k9vW-AWSGlueISArtifact-1.0.0.jar,/tmp/glue-job-7289043572118488777/jars/yYzKlj-AwsGlueMLLibs.jar,file:///tmp/glue-job-7289043572118488777/jars/liLJjV-aws-glue-di-package-5.0.296.jar,file:///tmp/glue-job-7289043572118488777/jars/b5k9vW-AWSGlueISArtifact-1.0.0.jar,file:///tmp/glue-job-7289043572118488777/jars/yYzKlj-AwsGlueMLLibs.jar,/glue/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,/glue/usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar,/glue/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar,/glue/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar,/glue/usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar,/glue/usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar,/usr/lib/spark/jars/datanucleus-api-jdo-4.2.4.jar,/usr/lib/spark/jars/datanucleus-core-4.1.17.jar,/usr/lib/spark/jars/datanucleus-rdbms-4.1.19.jar'), ('spark.resourceManager.cleanupExpiredHost', 'true'), ('spark.driver.bindAddress', '172.35.207.114'), ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'), ('spark.sql.warehouse.dir', 'file:/tmp/spark-warehouse'), ('spark.metrics.conf.*.sink.GlueCloudwatch.proxyHost', 'null'), ('spark.glue.GLUE_TASK_GROUP_ID', '47acefaa-7897-47d1-8f32-a4f9028c6eb0'), ('spark.glue.additionalParams.PROXY_DISABLED_V2', 'false'), ('spark.network.timeout', '600'), ('spark.eventLog.enabled', 'true'), ('spark.dynamicAllocation.minExecutors', '1'), ('spark.glue.additionalParams.PROXY_DISABLED', 'false'), ('spark.ui.showAdditionalMetrics', 'true'), ('spark.metrics.conf.*.source.system.class', 'org.apache.spark.metrics.source.SystemMetricsSource'), ('spark.eventLog.dir', 'file:///var/log/spark/apps'), ('spark.blacklist.decommissioning.timeout', '1h'), ('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions'), ('spark.hadoop.fs.s3a.committer.magic.track.commits.in.memory.enabled', 'true'), ('spark.hadoop.fs.s3.buffer.dir', '/tmp/hadoop/s3'), ('spark.history.fs.logDirectory', 'file:///var/log/spark/apps'), ('spark.metrics.conf.*.sink.GlueCloudwatch.accountId', '941377125894'), ('spark.glue.USE_PROXY', 'false'), ('spark.driver.extraJavaOptions', \"-Djava.net.preferIPv6Addresses=false -XX:OnOutOfMemoryError='kill -9 %p' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.glue.etl-enable-container-telemetry-collection', 'false'), ('spark.executor.id', 'driver'), ('spark.hadoop.hive.metastore.client.factory.class', 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'), ('spark.metrics.conf.driver.source.aggregate.class', 'org.apache.spark.metrics.source.AggregateMetricsSource'), ('spark.dynamicAllocation.maxExecutors', '1'), ('spark.sql.catalogImplementation', 'hive'), ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'), ('spark.executor.instances', '1'), ('spark.hadoop.fs.s3.customAWSCredentialsProvider', 'com.amazonaws.auth.DefaultAWSCredentialsProviderChain'), ('spark.metrics.conf.driver.source.glue.resourceUtilization.class', 'org.apache.spark.metrics.source.ResourceUtilizationMetricsSource'), ('spark.hadoop.fs.s3a.aws.credentials.provider', 'software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider'), ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'), ('spark.authenticate.secret', 'cc72b297-8f14-468f-8f51-5d738d813897'), ('spark.dynamicAllocation.enabled', 'false'), ('spark.authenticate', 'true'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2'), ('spark.app.name', '613a7f7e-0199-479e-9501-18fe0a567a11'), ('spark.shuffle.service.enabled', 'false'), ('spark.yarn.submit.waitAppCompletion', 'false'), ('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/goodies/lib/emr-serverless-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar'), ('spark.repl.class.uri', 'spark://172.35.207.114:43457/classes'), ('spark.hadoop.fs.defaultFS', 'file:///'), ('spark.hadoop.lakeformation.credentials.url', 'http://localhost:9998/lakeformationcredentials'), ('spark.app.startTime', '1736438090713'), ('spark.archives', '/tmp/glue-job-7289043572118488777_glue_venv.zip#python_environment'), ('spark.yarn.maxAppAttempts', '1'), ('spark.metrics.conf.driver.source.glue.jobPerformance.skewnessFactor', '5'), ('spark.glue.additionalParams.ADDITIONAL_GLUE_JDK_OPTS', '-Xmx10g -XX:+UseG1GC -XX:MaxHeapFreeRatio=70 -XX:InitiatingHeapOccupancyPercent=45'), ('spark.emr-serverless.client.create.batch.size', '100'), ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'), ('spark.metrics.conf.*.source.s3.class', 'org.apache.spark.metrics.source.S3FileSystemSource'), ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true'), ('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.app.initial.jar.urls', 'spark://172.35.207.114:43457/jars/commons-codec-1.9.jar,spark://172.35.207.114:43457/jars/datanucleus-rdbms-4.1.19.jar,spark://172.35.207.114:43457/jars/livy-repl_2.12-0.8.0-incubating.jar,spark://172.35.207.114:43457/jars/datanucleus-core-4.1.17.jar,spark://172.35.207.114:43457/jars/kryo-shaded-4.0.2.jar,spark://172.35.207.114:43457/jars/livy-core_2.12-0.8.0-incubating.jar,spark://172.35.207.114:43457/jars/minlog-1.3.0.jar,spark://172.35.207.114:43457/jars/b5k9vW-AWSGlueISArtifact-1.0.0.jar,spark://172.35.207.114:43457/jars/datanucleus-api-jdo-4.2.4.jar,spark://172.35.207.114:43457/jars/objenesis-2.5.1.jar,spark://172.35.207.114:43457/jars/liLJjV-aws-glue-di-package-5.0.296.jar,spark://172.35.207.114:43457/jars/yYzKlj-AwsGlueMLLibs.jar'), ('spark.livy.spark_major_version', '3'), ('spark.history.ui.port', '18080'), ('spark.metrics.conf.*.sink.GlueCloudwatch.namespace', 'Glue'), ('spark.executor.memory', '10g'), ('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.driver.memory', '10g'), ('spark.glue.GLUE_COMMAND_CRITERIA', 'glueetl'), ('spark.executorEnv.PYTHONPATH', 'python_environment'), ('spark.glue.glue-jars-dir', '/tmp/glue-job-7289043572118488777/jars'), ('spark.glue.glue-libs-temp-dir-path', '/tmp/glue-job-7289043572118488777'), ('spark.submit.pyFiles', ''), ('spark.yarn.isPython', 'true'), ('spark.emr-serverless.client.release.batch.size', '100'), ('spark.files.useFetchCache', 'false')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Import libraries",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport datetime",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Loading data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "c_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/small/customers/')\nc_df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "1\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "c_df.schema",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "StructType([StructField('cust_id', StringType(), True), StructField('name', StringType(), True), StructField('age', StringType(), True), StructField('gender', StringType(), True), StructField('birthday', StringType(), True), StructField('zip', StringType(), True), StructField('city', StringType(), True)])\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "c_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------+---+------+----------+-----+------------+\n|   cust_id|          name|age|gender|  birthday|  zip|        city|\n+----------+--------------+---+------+----------+-----+------------+\n|C007YEYTX9|  Aaron Abbott| 34|Female| 7/13/1991|97823|      boston|\n|C00B971T1J|  Aaron Austin| 37|Female|12/16/2004|30332|     chicago|\n|C00WRSJF1Q|  Aaron Barnes| 29|Female| 3/11/1977|23451|      denver|\n|C01AZWQMF3| Aaron Barrett| 31|  Male|  7/9/1998|46613| los_angeles|\n|C01BKUFRHA|  Aaron Becker| 54|  Male|11/24/1979|40284|   san_diego|\n|C01RGUNJV9|    Aaron Bell| 24|Female| 8/16/1968|86331|      denver|\n|C01USDV4EE|   Aaron Blair| 35|Female|  9/9/1974|80078|    new_york|\n|C01WMZQ7PN|   Aaron Brady| 51|Female| 8/20/1994|52204|philadelphia|\n|C021567NJZ|  Aaron Briggs| 57|  Male| 3/10/1990|22008|philadelphia|\n|C023M6MKR3|   Aaron Bryan| 29|  Male| 4/10/1976|05915|philadelphia|\n|C0248N0EK3|  Aaron Burton| 26|Female| 8/27/1964|50477| los_angeles|\n|C02C54RPNL|  Aaron Burton| 46|  Male| 5/29/1976|75857|     seattle|\n|C02ERIY1O4|  Aaron Cannon| 50|  Male| 5/23/1965|70209|    portland|\n|C02EVK2JWT|  Aaron Carter| 36|Female| 6/21/1993|89011|      denver|\n|C02JNTM46B|Aaron Chambers| 51|  Male|  1/6/2001|63337|    new_york|\n|C030A69V1L|  Aaron Clarke| 55|  Male| 4/28/1999|77176|philadelphia|\n|C033JBNUYU|Aaron Ferguson| 27|  Male| 6/21/1959|73150|      denver|\n|C034RB2MQ6|    Aaron Ford| 63|  Male|  7/8/1988|90592|     chicago|\n|C036GAJ3BV|Aaron Franklin| 22|Female| 3/14/1961|01187|   san_diego|\n|C03U340T3R| Aaron Gardner| 59|Female| 3/18/1975|31502|      denver|\n+----------+--------------+---+------+----------+-----+------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/small/transactions/')\nt_df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "13\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|  amt|         city|\n+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TM2DBM2C4ZRGDMK|2020-09-05|2020|    9|  5|Entertainment| 4.23| philadelphia|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T9OJV04D72A2TZY|2017-08-16|2017|    8| 16|Entertainment| 6.51|       denver|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TURTALJ1D3ESXWO|2016-12-19|2016|   12| 19| Motor/Travel|30.06|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TCWR5KX1P7KXH8X|2019-10-07|2019|   10|  7|Entertainment| 7.01|      chicago|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TBFPYQ01GHQ2M4K|2013-04-17|2013|    4| 17|Entertainment| 8.64| philadelphia|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T2YCJ2W1ZU1LGIM|2014-05-15|2014|    5| 15|Entertainment| 5.01|      seattle|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TJLU13B9ZZEA8OX|2016-07-24|2016|    7| 24|Entertainment| 7.06|    san_diego|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TMMI6HAPMB8QZOC|2012-05-27|2012|    5| 27|Entertainment|  4.3|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TAAF9Q62KV595EN|2019-12-07|2019|   12|  7|Entertainment| 5.94|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T0PLNEFX7VSNPT1|2018-11-24|2018|   11| 24| Motor/Travel| 16.5|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TPEHGVB1FIKAXUB|2020-05-17|2020|    5| 17|Entertainment|  4.3|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TVE4C79UIONM9QY|2014-09-09|2014|    9|  9|Entertainment| 4.53|      seattle|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TETU9XBFJQ2T9AS|2015-03-13|2015|    3| 13|Entertainment|19.31|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T3BA9U28MGJ2A2F|2012-10-09|2012|   10|  9| Motor/Travel| 4.98|     portland|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TEE5MRY94CLAAT4|2019-11-23|2019|   11| 23|    Groceries| 5.05|    san_diego|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TXRYEMOLOLWV3HF|2013-09-14|2013|    9| 14|Entertainment| 4.95|san_francisco|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TDU2WP5WWG7Q4ZE|2016-11-13|2016|   11| 13|     Clothing| 3.45|     portland|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TLGK5WSBK70GRF7|2018-05-10|2018|    5| 10|    Groceries| 3.81|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TN4VFFWBTW7VXJ6|2011-04-18|2011|    4| 18| Motor/Travel| 6.22|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TO27GZXG3Q6QMFL|2012-12-16|2012|   12| 16|Entertainment| 11.7|       denver|\n+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Examples of narrow transformations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "tmp_01_df = c_df.withColumn('first_name', F.split('name', ' ').getItem(0))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "tmp_02_df = tmp_01_df.withColumn('last_name', F.split('name', ' ').getItem(1))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "tmp_03_df = tmp_02_df.select(['cust_id', 'first_name', 'last_name', 'city', 'gender', 'birthday'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "tmp_04_df = tmp_03_df.filter(F.col('city') == 'chicago')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "tmp_04_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+---------+-------+------+----------+\n|   cust_id|first_name|last_name|   city|gender|  birthday|\n+----------+----------+---------+-------+------+----------+\n|C00B971T1J|     Aaron|   Austin|chicago|Female|12/16/2004|\n|C034RB2MQ6|     Aaron|     Ford|chicago|  Male|  7/8/1988|\n|C08RJGXUDH|     Aaron|  Jimenez|chicago|Female| 2/26/1982|\n|C0EF3QYJVK|     Aaron|   Thomas|chicago|Female| 4/27/1974|\n|C0FSON7A4J|     Abbie|    Burke|chicago|  Male| 3/28/1976|\n|C0HCX8JU7B|     Abbie|  Edwards|chicago|  Male| 2/21/1998|\n|C0LF7GK36H|     Abbie|    Lucas|chicago|  Male| 4/29/1961|\n|C0VK4CF81S|       Ada|    Casey|chicago|  Male| 6/14/1972|\n|C0VTJ1GFVP|       Ada|   Castro|chicago|  Male| 1/23/1969|\n|C107K4GY1L|       Ada|     Reed|chicago|  Male| 1/28/1987|\n|C12G5ORP07|       Ada|   Torres|chicago|Female|10/24/1964|\n|C13UV4JLH1|       Ada|    Woods|chicago|Female| 4/22/2000|\n|C146M4WYYT|      Adam| Alvarado|chicago|Female| 6/17/1980|\n|C16UWLYOBC|      Adam|     Bush|chicago|Female| 6/24/1991|\n|C1AVQUGTJB|      Adam|   Haynes|chicago|Female| 6/17/2003|\n|C1CO1M5YUX|      Adam|Maldonado|chicago|  Male|12/15/1961|\n|C1DE56NVW0|      Adam|   Newman|chicago|  Male| 4/25/2002|\n|C1FEEHNCFH|      Adam|  Wallace|chicago|Female| 2/20/1964|\n|C1G8RFWA8V|      Adam|    Watts|chicago|  Male|11/28/1992|\n|C1L8Q5X878|     Addie|     Hall|chicago|Female| 2/18/2005|\n+----------+----------+---------+-------+------+----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "tmp_04_df.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Filter ('city = chicago)\n+- Project [cust_id#0, first_name#112, last_name#121, city#6, gender#3, birthday#4]\n   +- Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, first_name#112, split(name#1,  , -1)[1] AS last_name#121]\n      +- Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, split(name#1,  , -1)[0] AS first_name#112]\n         +- Relation [cust_id#0,name#1,age#2,gender#3,birthday#4,zip#5,city#6] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, first_name: string, last_name: string, city: string, gender: string, birthday: string\nFilter (city#6 = chicago)\n+- Project [cust_id#0, first_name#112, last_name#121, city#6, gender#3, birthday#4]\n   +- Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, first_name#112, split(name#1,  , -1)[1] AS last_name#121]\n      +- Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, split(name#1,  , -1)[0] AS first_name#112]\n         +- Relation [cust_id#0,name#1,age#2,gender#3,birthday#4,zip#5,city#6] parquet\n\n== Optimized Logical Plan ==\nProject [cust_id#0, split(name#1,  , -1)[0] AS first_name#112, split(name#1,  , -1)[1] AS last_name#121, city#6, gender#3, birthday#4]\n+- Filter (isnotnull(city#6) AND (city#6 = chicago))\n   +- Relation [cust_id#0,name#1,age#2,gender#3,birthday#4,zip#5,city#6] parquet\n\n== Physical Plan ==\n*(1) Project [cust_id#0, split(name#1,  , -1)[0] AS first_name#112, split(name#1,  , -1)[1] AS last_name#121, city#6, gender#3, birthday#4]\n+- *(1) Filter (isnotnull(city#6) AND (city#6 = chicago))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet [cust_id#0,name#1,gender#3,birthday#4,city#6] Batched: true, DataFilters: [isnotnull(city#6), (city#6 = chicago)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/customers], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,chicago)], ReadSchema: struct<cust_id:string,name:string,gender:string,birthday:string,city:string>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Parsed plan\n```bash\n== Parsed Logical Plan ==\n'Project ['cust_id, 'first_name, 'last_name, 'city, 'gender, 'birthday]\n+- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, first_name#579, split(name#89,  , -1)[1] AS last_name#588]\n   +- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, split(name#89,  , -1)[0] AS first_name#579]\n      +- Filter (city#94 = chicago)\n         +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n```\n#### Logical plan\n```bash\n== Analyzed Logical Plan ==\ncust_id: string, first_name: string, last_name: string, city: string, gender: string, birthday: string\nProject [cust_id#88, first_name#579, last_name#588, city#94, gender#91, birthday#92]\n+- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, first_name#579, split(name#89,  , -1)[1] AS last_name#588]\n   +- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, split(name#89,  , -1)[0] AS first_name#579]\n      +- Filter (city#94 = chicago)\n         +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n```\n#### Optimized plan\n```bash\n== Optimized Logical Plan ==\nProject [cust_id#88, split(name#89,  , -1)[0] AS first_name#579, split(name#89,  , -1)[1] AS last_name#588, city#94, gender#91, birthday#92]\n+- Filter (isnotnull(city#94) AND (city#94 = chicago))\n   +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n```\n#### Physical plan\n```bash\n== Physical Plan ==\n*(1) Project [cust_id#88, split(name#89,  , -1)[0] AS first_name#579, split(name#89,  , -1)[1] AS last_name#588, city#94, gender#91, birthday#92]\n+- *(1) Filter (isnotnull(city#94) AND (city#94 = chicago))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet [cust_id#88,name#89,gender#91,birthday#92,city#94] Batched: true, DataFilters: [isnotnull(city#94), (city#94 = chicago)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/customers], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,chicago)], ReadSchema: struct<cust_id:string,name:string,gender:string,birthday:string,city:string>\n```",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "### Examples of wide transformations",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Repartition",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "t_df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "13\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.repartition(20).explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\nRepartition 20, true\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartition 20, true\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Optimized Logical Plan ==\nRepartition 20, true\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange RoundRobinPartitioning(20), REPARTITION_BY_NUM, [plan_id=297]\n   +- FileScan parquet [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.repartition('city').explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'RepartitionByExpression ['city]\n+- Relation [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartitionByExpression [city#54]\n+- Relation [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] parquet\n\n== Optimized Logical Plan ==\nRepartitionByExpression [city#54]\n+- Relation [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange hashpartitioning(city#54, 4), REPARTITION_BY_COL, [plan_id=90]\n   +- FileScan parquet [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.repartition('city').rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.repartition(2, 'city').explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'RepartitionByExpression ['city], 2\n+- Relation [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartitionByExpression [city#54], 2\n+- Relation [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] parquet\n\n== Optimized Logical Plan ==\nRepartitionByExpression [city#54], 2\n+- Relation [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange hashpartitioning(city#54, 2), REPARTITION_BY_NUM, [plan_id=125]\n   +- FileScan parquet [cust_id#44,start_date#45,end_date#46,txn_id#47,date#48,year#49,month#50,day#51,expense_type#52,amt#53,city#54] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Coalesce",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "t_df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "13\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.coalesce(4).rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.coalesce(4).explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\nRepartition 4, false\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartition 4, false\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Optimized Logical Plan ==\nRepartition 4, false\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Physical Plan ==\nCoalesce 4\n+- *(1) ColumnarToRow\n   +- FileScan parquet [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.coalesce(1).explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\nRepartition 1, false\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartition 1, false\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Optimized Logical Plan ==\nRepartition 1, false\n+- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n\n== Physical Plan ==\nCoalesce 1\n+- *(1) ColumnarToRow\n   +- FileScan parquet [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Join",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "t_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|  amt|         city|\n+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TM2DBM2C4ZRGDMK|2020-09-05|2020|    9|  5|Entertainment| 4.23| philadelphia|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T9OJV04D72A2TZY|2017-08-16|2017|    8| 16|Entertainment| 6.51|       denver|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TURTALJ1D3ESXWO|2016-12-19|2016|   12| 19| Motor/Travel|30.06|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TCWR5KX1P7KXH8X|2019-10-07|2019|   10|  7|Entertainment| 7.01|      chicago|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TBFPYQ01GHQ2M4K|2013-04-17|2013|    4| 17|Entertainment| 8.64| philadelphia|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T2YCJ2W1ZU1LGIM|2014-05-15|2014|    5| 15|Entertainment| 5.01|      seattle|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TJLU13B9ZZEA8OX|2016-07-24|2016|    7| 24|Entertainment| 7.06|    san_diego|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TMMI6HAPMB8QZOC|2012-05-27|2012|    5| 27|Entertainment|  4.3|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TAAF9Q62KV595EN|2019-12-07|2019|   12|  7|Entertainment| 5.94|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T0PLNEFX7VSNPT1|2018-11-24|2018|   11| 24| Motor/Travel| 16.5|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TPEHGVB1FIKAXUB|2020-05-17|2020|    5| 17|Entertainment|  4.3|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TVE4C79UIONM9QY|2014-09-09|2014|    9|  9|Entertainment| 4.53|      seattle|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TETU9XBFJQ2T9AS|2015-03-13|2015|    3| 13|Entertainment|19.31|  los_angeles|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|T3BA9U28MGJ2A2F|2012-10-09|2012|   10|  9| Motor/Travel| 4.98|     portland|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TEE5MRY94CLAAT4|2019-11-23|2019|   11| 23|    Groceries| 5.05|    san_diego|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TXRYEMOLOLWV3HF|2013-09-14|2013|    9| 14|Entertainment| 4.95|san_francisco|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TDU2WP5WWG7Q4ZE|2016-11-13|2016|   11| 13|     Clothing| 3.45|     portland|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TLGK5WSBK70GRF7|2018-05-10|2018|    5| 10|    Groceries| 3.81|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TN4VFFWBTW7VXJ6|2011-04-18|2011|    4| 18| Motor/Travel| 6.22|     new_york|\n|C0YDPQWPBJ|2010-12-01|2020-11-01|TO27GZXG3Q6QMFL|2012-12-16|2012|   12| 16|Entertainment| 11.7|       denver|\n+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "c_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------+---+------+----------+-----+------------+\n|   cust_id|          name|age|gender|  birthday|  zip|        city|\n+----------+--------------+---+------+----------+-----+------------+\n|C007YEYTX9|  Aaron Abbott| 34|Female| 7/13/1991|97823|      boston|\n|C00B971T1J|  Aaron Austin| 37|Female|12/16/2004|30332|     chicago|\n|C00WRSJF1Q|  Aaron Barnes| 29|Female| 3/11/1977|23451|      denver|\n|C01AZWQMF3| Aaron Barrett| 31|  Male|  7/9/1998|46613| los_angeles|\n|C01BKUFRHA|  Aaron Becker| 54|  Male|11/24/1979|40284|   san_diego|\n|C01RGUNJV9|    Aaron Bell| 24|Female| 8/16/1968|86331|      denver|\n|C01USDV4EE|   Aaron Blair| 35|Female|  9/9/1974|80078|    new_york|\n|C01WMZQ7PN|   Aaron Brady| 51|Female| 8/20/1994|52204|philadelphia|\n|C021567NJZ|  Aaron Briggs| 57|  Male| 3/10/1990|22008|philadelphia|\n|C023M6MKR3|   Aaron Bryan| 29|  Male| 4/10/1976|05915|philadelphia|\n|C0248N0EK3|  Aaron Burton| 26|Female| 8/27/1964|50477| los_angeles|\n|C02C54RPNL|  Aaron Burton| 46|  Male| 5/29/1976|75857|     seattle|\n|C02ERIY1O4|  Aaron Cannon| 50|  Male| 5/23/1965|70209|    portland|\n|C02EVK2JWT|  Aaron Carter| 36|Female| 6/21/1993|89011|      denver|\n|C02JNTM46B|Aaron Chambers| 51|  Male|  1/6/2001|63337|    new_york|\n|C030A69V1L|  Aaron Clarke| 55|  Male| 4/28/1999|77176|philadelphia|\n|C033JBNUYU|Aaron Ferguson| 27|  Male| 6/21/1959|73150|      denver|\n|C034RB2MQ6|    Aaron Ford| 63|  Male|  7/8/1988|90592|     chicago|\n|C036GAJ3BV|Aaron Franklin| 22|Female| 3/14/1961|01187|   san_diego|\n|C03U340T3R| Aaron Gardner| 59|Female| 3/18/1975|31502|      denver|\n+----------+--------------+---+------+----------+-----+------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# TEMPORARY disable auto broadcast join\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 66,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "joined_df = t_df.join(\n    other=c_df,\n    how='inner',\n    on='cust_id')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 67,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "joined.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "joined.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 65,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Join UsingJoin(Inner, [cust_id])\n:- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n+- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\nProject [cust_id#102, start_date#103, end_date#104, txn_id#105, date#106, year#107, month#108, day#109, expense_type#110, amt#111, city#112, name#89, age#90, gender#91, birthday#92, zip#93, city#94]\n+- Join Inner, (cust_id#102 = cust_id#88)\n   :- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n   +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n\n== Optimized Logical Plan ==\nProject [cust_id#102, start_date#103, end_date#104, txn_id#105, date#106, year#107, month#108, day#109, expense_type#110, amt#111, city#112, name#89, age#90, gender#91, birthday#92, zip#93, city#94]\n+- Join Inner, (cust_id#102 = cust_id#88)\n   :- Filter isnotnull(cust_id#102)\n   :  +- Relation [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] parquet\n   +- Filter isnotnull(cust_id#88)\n      +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [cust_id#102, start_date#103, end_date#104, txn_id#105, date#106, year#107, month#108, day#109, expense_type#110, amt#111, city#112, name#89, age#90, gender#91, birthday#92, zip#93, city#94]\n   +- SortMergeJoin [cust_id#102], [cust_id#88], Inner\n      :- Sort [cust_id#102 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(cust_id#102, 4), ENSURE_REQUIREMENTS, [plan_id=698]\n      :     +- Filter isnotnull(cust_id#102)\n      :        +- FileScan parquet [cust_id#102,start_date#103,end_date#104,txn_id#105,date#106,year#107,month#108,day#109,expense_type#110,amt#111,city#112] Batched: true, DataFilters: [isnotnull(cust_id#102)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n      +- Sort [cust_id#88 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(cust_id#88, 4), ENSURE_REQUIREMENTS, [plan_id=699]\n            +- Filter isnotnull(cust_id#88)\n               +- FileScan parquet [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] Batched: true, DataFilters: [isnotnull(cust_id#88)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/customers], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### GroupBy",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "g_df = t_df.groupBy('cust_id').agg({'txn_id': 'count', 'amt': 'sum'})",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "g_df.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Aggregate ['cust_id], ['cust_id, count(txn_id#61) AS count(txn_id)#410L, unresolvedalias('sum(amt#67), None)]\n+- Relation [cust_id#58,start_date#59,end_date#60,txn_id#61,date#62,year#63,month#64,day#65,expense_type#66,amt#67,city#68] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, count(txn_id): bigint, sum(amt): double\nAggregate [cust_id#58], [cust_id#58, count(txn_id#61) AS count(txn_id)#410L, sum(cast(amt#67 as double)) AS sum(amt)#412]\n+- Relation [cust_id#58,start_date#59,end_date#60,txn_id#61,date#62,year#63,month#64,day#65,expense_type#66,amt#67,city#68] parquet\n\n== Optimized Logical Plan ==\nAggregate [cust_id#58], [cust_id#58, count(txn_id#61) AS count(txn_id)#410L, sum(cast(amt#67 as double)) AS sum(amt)#412]\n+- Project [cust_id#58, txn_id#61, amt#67]\n   +- Relation [cust_id#58,start_date#59,end_date#60,txn_id#61,date#62,year#63,month#64,day#65,expense_type#66,amt#67,city#68] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[cust_id#58], functions=[count(txn_id#61), sum(cast(amt#67 as double))], output=[cust_id#58, count(txn_id)#410L, sum(amt)#412], schema specialized)\n   +- Exchange hashpartitioning(cust_id#58, 8), ENSURE_REQUIREMENTS, [plan_id=275]\n      +- HashAggregate(keys=[cust_id#58], functions=[partial_count(txn_id#61), partial_sum(cast(amt#67 as double))], output=[cust_id#58, count#417L, sum#419], schema specialized)\n         +- FileScan parquet [cust_id#58,txn_id#61,amt#67] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,txn_id:string,amt:string>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "g_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-------------+------------------+\n|   cust_id|count(txn_id)|          sum(amt)|\n+----------+-------------+------------------+\n|CEVULXC7UN|         7146| 488571.1299999981|\n|CSN6AIYZG9|         7566|          760164.9|\n|CFSO1WP6OE|         7547| 891046.2500000012|\n|CWZQEDFVMJ|         7988| 836305.6000000015|\n|CBBGI50QZ1|         7953|  964384.620000003|\n|CDY1XBW4KD|         7485|1601169.1700000048|\n|C8R3WPMPE4|         7351| 842854.5299999998|\n|CBQNUAPTEE|         7678| 854887.0599999976|\n|CM60NRREKK|         7598| 189735.2500000001|\n|C2Q7HQMS29|         7184|132972.24999999994|\n|CP2WGJ7O5J|         7324| 198217.0900000002|\n|C4IN9KHAHZ|         7156|150539.09000000008|\n|CKVSGFJRQY|         7309|1302532.3599999994|\n|CAHGQXCG13|         7391| 792579.1599999988|\n|CL66HEUWQP|         7604|  571821.799999999|\n|CYVHSYX1C2|         7629| 955859.7599999976|\n|CRZCTBH6C2|         7462| 620124.5200000014|\n|C3KUDEN3KO|         7999| 1535998.699999997|\n|C1QEUH9YHR|         7408| 618350.4400000009|\n|CT2KSHI40E|         7565|         781501.75|\n+----------+-------------+------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.schema",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "StructType([StructField('cust_id', StringType(), True), StructField('start_date', StringType(), True), StructField('end_date', StringType(), True), StructField('txn_id', StringType(), True), StructField('date', StringType(), True), StructField('year', StringType(), True), StructField('month', StringType(), True), StructField('day', StringType(), True), StructField('expense_type', StringType(), True), StructField('amt', StringType(), True), StructField('city', StringType(), True)])\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.select('cust_id').distinct().count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "2958\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "t_df.repartition(64, 'cust_id').groupBy('cust_id').agg({'txn_id': 'count', 'amt': 'sum'}).explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Aggregate ['cust_id], ['cust_id, count(txn_id#61) AS count(txn_id)#488L, unresolvedalias('sum(amt#67), None)]\n+- RepartitionByExpression [cust_id#58], 64\n   +- Relation [cust_id#58,start_date#59,end_date#60,txn_id#61,date#62,year#63,month#64,day#65,expense_type#66,amt#67,city#68] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, count(txn_id): bigint, sum(amt): double\nAggregate [cust_id#58], [cust_id#58, count(txn_id#61) AS count(txn_id)#488L, sum(cast(amt#67 as double)) AS sum(amt)#490]\n+- RepartitionByExpression [cust_id#58], 64\n   +- Relation [cust_id#58,start_date#59,end_date#60,txn_id#61,date#62,year#63,month#64,day#65,expense_type#66,amt#67,city#68] parquet\n\n== Optimized Logical Plan ==\nAggregate [cust_id#58], [cust_id#58, count(txn_id#61) AS count(txn_id)#488L, sum(cast(amt#67 as double)) AS sum(amt)#490]\n+- RepartitionByExpression [cust_id#58], 64\n   +- Project [cust_id#58, txn_id#61, amt#67]\n      +- Relation [cust_id#58,start_date#59,end_date#60,txn_id#61,date#62,year#63,month#64,day#65,expense_type#66,amt#67,city#68] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[cust_id#58], functions=[count(txn_id#61), sum(cast(amt#67 as double))], output=[cust_id#58, count(txn_id)#488L, sum(amt)#490], schema specialized)\n   +- HashAggregate(keys=[cust_id#58], functions=[partial_count(txn_id#61), partial_sum(cast(amt#67 as double))], output=[cust_id#58, count#495L, sum#497], schema specialized)\n      +- Exchange hashpartitioning(cust_id#58, 64), REPARTITION_BY_NUM, [plan_id=471]\n         +- FileScan parquet [cust_id#58,txn_id#61,amt#67] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/transactions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,txn_id:string,amt:string>\n",
					"output_type": "stream"
				}
			]
		}
	]
}