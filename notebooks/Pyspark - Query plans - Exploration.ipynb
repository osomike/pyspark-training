{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# PySpark Training Notebook\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Run these cells to configure your interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%idle_timeout 60\n",
    "%glue_version 5.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"--enable-continuous-cloudwatch-log\": \"true\",\n",
    "    \"--enable-spark-ui\": \"true\",\n",
    "    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n",
    "    \"--enable-metrics\": \"true\",\n",
    "    \"--enable-observability-metrics\": \"true\",\n",
    "    \"--conf\": \"spark.sql.codegen.comments=true\",\n",
    "    \"--conf\": \"spark.sql.codegen.fallback=true\",\n",
    "    \"--conf\": \"spark.sql.codegen.wholeStage=true\",\n",
    "    \"--conf\": \"spark.sql.ui.explainMode=extended\",\n",
    "    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n",
    "    \"--conf\": \"spark.ui.retainedJobs=1000\",\n",
    "    \"--conf\": \"spark.ui.retainedStages=1000\",\n",
    "    \"--conf\": \"spark.ui.retainedTasks=10000\",\n",
    "    \"--conf\": \"spark.ui.showAdditionalMetrics=true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Start spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Get spark's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\n",
    "dynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\n",
    "dynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\n",
    "dynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n",
    "\n",
    "executor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\n",
    "executor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\n",
    "executor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n",
    "\n",
    "driver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\n",
    "driver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n",
    "\n",
    "print(f'''\n",
    "Dynamic allocation enabled: {dynamic_allocation_enabled}\n",
    "Dynamic min executors: {dynamic_min_executors}\n",
    "Dynamic max executors: {dynamic_max_executors}\n",
    "Dynamic initial executors: {dynamic_initial_executors}\n",
    "----------------------------------------\n",
    "Executor instances: {executor_instances}\n",
    "Executor cores: {executor_cores}\n",
    "Executor memory: {executor_memory}\n",
    "----------------------------------------\n",
    "Driver cores: {driver_cores}\n",
    "Driver memory: {driver_memory}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading customers and transaction datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/small/customers/')\n",
    "c_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df = spark.read.format('parquet').load('s3://dip-pyspark-training/data/small/transactions/')\n",
    "t_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of narrow transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_01_df = c_df.withColumn('first_name', F.split('name', ' ').getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_02_df = tmp_01_df.withColumn('last_name', F.split('name', ' ').getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_03_df = tmp_02_df.select(['cust_id', 'first_name', 'last_name', 'city', 'gender', 'birthday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_04_df = tmp_03_df.filter(F.col('city') == 'chicago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_04_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_04_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parsed plan\n",
    "```bash\n",
    "== Parsed Logical Plan ==\n",
    "'Project ['cust_id, 'first_name, 'last_name, 'city, 'gender, 'birthday]\n",
    "+- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, first_name#579, split(name#89,  , -1)[1] AS last_name#588]\n",
    "   +- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, split(name#89,  , -1)[0] AS first_name#579]\n",
    "      +- Filter (city#94 = chicago)\n",
    "         +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n",
    "```\n",
    "#### Logical plan\n",
    "```bash\n",
    "== Analyzed Logical Plan ==\n",
    "cust_id: string, first_name: string, last_name: string, city: string, gender: string, birthday: string\n",
    "Project [cust_id#88, first_name#579, last_name#588, city#94, gender#91, birthday#92]\n",
    "+- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, first_name#579, split(name#89,  , -1)[1] AS last_name#588]\n",
    "   +- Project [cust_id#88, name#89, age#90, gender#91, birthday#92, zip#93, city#94, split(name#89,  , -1)[0] AS first_name#579]\n",
    "      +- Filter (city#94 = chicago)\n",
    "         +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n",
    "```\n",
    "#### Optimized plan\n",
    "```bash\n",
    "== Optimized Logical Plan ==\n",
    "Project [cust_id#88, split(name#89,  , -1)[0] AS first_name#579, split(name#89,  , -1)[1] AS last_name#588, city#94, gender#91, birthday#92]\n",
    "+- Filter (isnotnull(city#94) AND (city#94 = chicago))\n",
    "   +- Relation [cust_id#88,name#89,age#90,gender#91,birthday#92,zip#93,city#94] parquet\n",
    "```\n",
    "#### Physical plan\n",
    "```bash\n",
    "== Physical Plan ==\n",
    "*(1) Project [cust_id#88, split(name#89,  , -1)[0] AS first_name#579, split(name#89,  , -1)[1] AS last_name#588, city#94, gender#91, birthday#92]\n",
    "+- *(1) Filter (isnotnull(city#94) AND (city#94 = chicago))\n",
    "   +- *(1) ColumnarToRow\n",
    "      +- FileScan parquet [cust_id#88,name#89,gender#91,birthday#92,city#94] Batched: true, DataFilters: [isnotnull(city#94), (city#94 = chicago)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/data/small/customers], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,chicago)], ReadSchema: struct<cust_id:string,name:string,gender:string,birthday:string,city:string>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examples of wide transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.repartition(20).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.repartition('city').explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.repartition('city').rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.repartition(2, 'city').explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.coalesce(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.coalesce(4).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.coalesce(1).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEMPORARY disable auto broadcast join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined_df = t_df.join(\n",
    "    other=c_df,\n",
    "    how='inner',\n",
    "    on='cust_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_df = t_df.groupBy('cust_id').agg({'txn_id': 'count', 'amt': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.select('cust_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_df.repartition(64, 'cust_id').groupBy('cust_id').agg({'txn_id': 'count', 'amt': 'sum'}).explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
