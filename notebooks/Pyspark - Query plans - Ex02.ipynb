{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# PySpark Training Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "\n####  Run this cell to set up and start your interactive session.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 4",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 3\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--enable-continuous-cloudwatch-log\": \"true\",\n    \"--enable-spark-ui\": \"true\",\n    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\",\n    \"--enable-metrics\": \"true\",\n    \"--enable-observability-metrics\": \"true\",\n    \"--conf\": \"spark.sql.codegen.comments=true\",\n    \"--conf\": \"spark.sql.codegen.fallback=true\",\n    \"--conf\": \"spark.sql.codegen.wholeStage=true\",\n    \"--conf\": \"spark.sql.ui.explainMode=extended\",\n    \"--conf\": \"spark.sql.ui.retainedExecutions=100\",\n    \"--conf\": \"spark.ui.retainedJobs=1000\",\n    \"--conf\": \"spark.ui.retainedStages=1000\",\n    \"--conf\": \"spark.ui.retainedTasks=10000\",\n    \"--conf\": \"spark.ui.showAdditionalMetrics=true\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://dip-pyspark-training/spark_ui_tmp/', '--enable-metrics': 'true', '--enable-observability-metrics': 'true', '--conf': 'spark.sql.codegen.comments=true'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 3\nIdle Timeout: 60\nSession ID: 90bb883e-a75d-44fd-9fa3-384021c6ee2b\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--enable-spark-ui true\n--spark-event-logs-path s3://dip-pyspark-training/spark_ui_tmp/\n--enable-metrics true\n--enable-observability-metrics true\n--conf spark.sql.codegen.comments=true\nWaiting for session 90bb883e-a75d-44fd-9fa3-384021c6ee2b to get into ready status...\nSession 90bb883e-a75d-44fd-9fa3-384021c6ee2b has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Get spark configuration\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dynamic_allocation_enabled = spark.sparkContext.getConf().get('spark.dynamicAllocation.enabled')\ndynamic_min_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.minExecutors')\ndynamic_max_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.maxExecutors')\ndynamic_initial_executors = spark.sparkContext.getConf().get('spark.dynamicAllocation.initialExecutors')\n\nexecutor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\nexecutor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\nexecutor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n\ndriver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\ndriver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n\nprint(f'''\nDynamic allocation enabled: {dynamic_allocation_enabled}\nDynamic min executors: {dynamic_min_executors}\nDynamic max executors: {dynamic_max_executors}\nDynamic initial executors: {dynamic_initial_executors}\n----------------------------------------\nExecutor instances: {executor_instances}\nExecutor cores: {executor_cores}\nExecutor memory: {executor_memory}\n----------------------------------------\nDriver cores: {driver_cores}\nDriver memory: {driver_memory}\n''')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\nDynamic allocation enabled: false\nDynamic min executors: 1\nDynamic max executors: 2\nDynamic initial executors: 3\n----------------------------------------\nExecutor instances: 2\nExecutor cores: 4\nExecutor memory: 10g\n----------------------------------------\nDriver cores: 4\nDriver memory: 10g\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Import libraries",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport datetime",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format('parquet').load('s3://dip-pyspark-training/ny-taxi-dataset-partitioned/')\ndf.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "432\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#df.select('vendor_id').distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Define the data as lists\nvendors = ['VTS', 'CMT', 'DDS', 'VTS', 'CMT', 'DDS']\npayment_type = ['CASH', 'CASH', 'CASH', 'CREDIT', 'CREDIT', 'CREDIT']\nextra_col = ['A', 'B', 'C', 'D', 'E', 'F']\n\n# Define the schema of the dataframe\nschema = T.StructType([\n    T.StructField(\"vendor_id\", T.StringType(), False),\n    T.StructField(\"payment_type\", T.StringType(), False),\n    T.StructField(\"extra_col_from_m\", T.StringType(), False)\n])\n\n# Create a list of tuples\ndata = [(vendors[i], payment_type[i], extra_col[i]) for i in range(len(vendors))]\n\n# Create a PySpark dataframe\nm_df = spark.createDataFrame(data, schema)\n#m_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#joined_df = df.join(other=m_df.hint('broadcast'), how='left', on = ['vendor_id', 'payment_type']).filter(F.col('extra_col_from_m').isNotNull())\njoined_df = df.join(other=m_df.hint('broadcast'), how='left', on = ['vendor_id', 'payment_type']).filter(F.col('extra_col_from_m').isNotNull())\njoined_df.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Filter isnotnull('extra_col_from_m)\n+- Project [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n   +- Join LeftOuter, ((vendor_id#17 = vendor_id#36) AND (payment_type#10 = payment_type#37))\n      :- Relation [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] parquet\n      +- ResolvedHint (strategy=broadcast)\n         +- LogicalRDD [vendor_id#36, payment_type#37, extra_col_from_m#38], false\n\n== Analyzed Logical Plan ==\nvendor_id: string, payment_type: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, trip_distance: double, pickup_longitude: double, pickup_latitude: double, rate_code_id: int, store_and_fwd_flag: string, dropoff_longitude: double, dropoff_latitude: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, total_amount: double, extra_col_from_m: string\nFilter isnotnull(extra_col_from_m#38)\n+- Project [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n   +- Join LeftOuter, ((vendor_id#17 = vendor_id#36) AND (payment_type#10 = payment_type#37))\n      :- Relation [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] parquet\n      +- ResolvedHint (strategy=broadcast)\n         +- LogicalRDD [vendor_id#36, payment_type#37, extra_col_from_m#38], false\n\n== Optimized Logical Plan ==\nProject [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n+- Join Inner, ((vendor_id#17 = vendor_id#36) AND (payment_type#10 = payment_type#37)), rightHint=(strategy=broadcast)\n   :- Filter (isnotnull(vendor_id#17) AND isnotnull(payment_type#10))\n   :  +- Relation [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] parquet\n   +- Filter isnotnull(extra_col_from_m#38)\n      +- LogicalRDD [vendor_id#36, payment_type#37, extra_col_from_m#38], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [vendor_id#17, payment_type#10, pickup_datetime#0, dropoff_datetime#1, passenger_count#2, trip_distance#3, pickup_longitude#4, pickup_latitude#5, rate_code_id#6, store_and_fwd_flag#7, dropoff_longitude#8, dropoff_latitude#9, fare_amount#11, extra#12, mta_tax#13, tip_amount#14, tolls_amount#15, total_amount#16, extra_col_from_m#38]\n   +- BroadcastHashJoin [vendor_id#17, payment_type#10], [vendor_id#36, payment_type#37], Inner, BuildRight, false\n      :- Filter isnotnull(payment_type#10)\n      :  +- FileScan parquet [pickup_datetime#0,dropoff_datetime#1,passenger_count#2,trip_distance#3,pickup_longitude#4,pickup_latitude#5,rate_code_id#6,store_and_fwd_flag#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,extra#12,mta_tax#13,tip_amount#14,tolls_amount#15,total_amount#16,vendor_id#17] Batched: true, DataFilters: [isnotnull(payment_type#10)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://dip-pyspark-training/ny-taxi-dataset-partitioned], PartitionFilters: [isnotnull(vendor_id#17)], PushedFilters: [IsNotNull(payment_type)], ReadSchema: struct<pickup_datetime:timestamp,dropoff_datetime:timestamp,passenger_count:int,trip_distance:dou...\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=35]\n         +- Filter isnotnull(extra_col_from_m#38)\n            +- Scan ExistingRDD[vendor_id#36,payment_type#37,extra_col_from_m#38]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "merged_df.write.format('parquet').mode('overwrite').save('s3://dip-pyspark-training/dummy-output-03/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}