{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# PySpark Training Notebook\n##### Refreshing of basic concepts",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run these cells to configure your interactive session",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--spark-event-logs-path\": \"s3://dip-pyspark-training/spark_ui_tmp/\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--spark-event-logs-path': 's3://dip-pyspark-training/spark_ui_tmp/'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Start spark session ",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 60\nSession ID: 6abe1590-24b2-489e-96b2-fe8bf106612f\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--spark-event-logs-path s3://dip-pyspark-training/spark_ui_tmp/\nWaiting for session 6abe1590-24b2-489e-96b2-fe8bf106612f to get into ready status...\nSession 6abe1590-24b2-489e-96b2-fe8bf106612f has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Spark's Core components",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "executor_instances = spark.sparkContext.getConf().get('spark.executor.instances')\nexecutor_cores = spark.sparkContext.getConf().get('spark.executor.cores')\nexecutor_memory = spark.sparkContext.getConf().get('spark.executor.memory')\n\ndriver_cores = spark.sparkContext.getConf().get('spark.driver.cores')\ndriver_memory = spark.sparkContext.getConf().get('spark.driver.memory')\n\nprint(f'''\n----------------------------------------\nExecutor instances: {executor_instances}\nExecutor cores: {executor_cores}\nExecutor memory: {executor_memory}\n----------------------------------------\nDriver cores: {driver_cores}\nDriver memory: {driver_memory}\n----------------------------------------\n''')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n----------------------------------------\nExecutor instances: 1\nExecutor cores: 4\nExecutor memory: 10g\n----------------------------------------\nDriver cores: 4\nDriver memory: 10g\n----------------------------------------\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sparkContext.getConf().getAll()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "[('spark.network.timeout', '600'), ('spark.files.useFetchCache', 'false'), ('spark.dynamicAllocation.minExecutors', '1'), ('spark.yarn.dist.archives', 'file:///tmp/glue-job-13064934523853194364_glue_venv.zip#python_environment'), ('spark.driver.cores', '4'), ('spark.glue.additionalParams.PROXY_DISABLED', 'false'), ('spark.hadoop.fs.AbstractFileSystem.s3.impl', 'org.apache.hadoop.fs.s3.EMRFSDelegate'), ('spark.eventLog.enabled', 'true'), ('spark.sql.shuffle.partitions', '4'), ('spark.eventLog.dir', 'file:///var/log/spark/apps'), ('spark.executor.extraJavaOptions', \"-Djava.net.preferIPv6Addresses=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.glue.additionalParams.ADDITIONAL_CLASSPATH', '/tmp/glue-job-13064934523853194364/*:/usr/lib/livy/rsc-jars/*:/usr/lib/livy/repl_2.12-jars/*:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/goodies/lib/emr-serverless-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar:/usr/lib/spark/jars/*:null/jars/*::/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/hadoop/*:/tmp/glue-job-13064934523853194364/jars/*:/tmp/glue-job-13064934523853194364/python/*:/tmp/glue-job-13064934523853194364/aws_glue_connectors/marketplace/*:/usr/share/aws/aws-glue-shuffle/AWSGlueShuffle-5.0.0.jar:/usr/share/aws/emr-glue-bootstrap/EMRGlueBootstrap-5.0-jar-with-dependencies.jar:/usr/lib/spark/jars/HikariCP-2.5.1.jar:/usr/lib/spark/jars/JLargeArrays-1.5.jar:/usr/lib/spark/jars/JTransforms-3.1.jar:/usr/lib/spark/jars/RoaringBitmap-0.9.45.jar:/usr/lib/spark/jars/ST4-4.0.4.jar:/usr/lib/spark/jars/activation-1.1.1.jar:/usr/lib/spark/jars/aggdesigner-algorithm-6.0.jar:/usr/lib/spark/jars/aircompressor-0.27.jar:/usr/lib/spark/jars/algebra_2.12-2.0.1.jar:/usr/lib/spark/jars/animal-sniffer-annotations-1.23.jar:/usr/lib/spark/jars/annotations-17.0.0.jar:/usr/lib/spark/jars/annotations-4.1.1.4.jar:/usr/lib/spark/jars/antlr-runtime-3.5.2.jar:/usr/lib/spark/jars/antlr4-runtime-4.9.3.jar:/usr/lib/spark/jars/aopalliance-repackaged-2.6.1.jar:/usr/lib/spark/jars/arpack-3.0.3.jar:/usr/lib/spark/jars/arpack_combined_all-0.1.jar:/usr/lib/spark/jars/arrow-format-12.0.1.jar:/usr/lib/spark/jars/arrow-memory-core-12.0.1.jar:/usr/lib/spark/jars/arrow-memory-netty-12.0.1.jar:/usr/lib/spark/jars/arrow-vector-12.0.1.jar:/usr/lib/spark/jars/audience-annotations-0.12.0.jar:/usr/lib/spark/jars/avro-1.11.2.jar:/usr/lib/spark/jars/avro-ipc-1.11.2.jar:/usr/lib/spark/jars/avro-mapred-1.11.2.jar:/usr/lib/spark/jars/blas-3.0.3.jar:/usr/lib/spark/jars/bonecp-0.8.0.RELEASE.jar:/usr/lib/spark/jars/breeze-macros_2.12-2.1.0.jar:/usr/lib/spark/jars/breeze_2.12-2.1.0.jar:/usr/lib/spark/jars/cats-kernel_2.12-2.1.1.jar:/usr/lib/spark/jars/chill-java-0.10.0.jar:/usr/lib/spark/jars/chill_2.12-0.10.0.jar:/usr/lib/spark/jars/commons-cli-1.5.0.jar:/usr/lib/spark/jars/commons-codec-1.16.1.jar:/usr/lib/spark/jars/commons-collections-3.2.2.jar:/usr/lib/spark/jars/commons-collections4-4.4.jar:/usr/lib/spark/jars/commons-compiler-3.1.9.jar:/usr/lib/spark/jars/commons-compress-1.23.0.jar:/usr/lib/spark/jars/commons-crypto-1.1.0.jar:/usr/lib/spark/jars/commons-dbcp-1.4.jar:/usr/lib/spark/jars/commons-io-2.16.1.jar:/usr/lib/spark/jars/commons-lang-2.6.jar:/usr/lib/spark/jars/commons-lang3-3.12.0.jar:/usr/lib/spark/jars/commons-logging-1.1.3.jar:/usr/lib/spark/jars/commons-math3-3.6.1.jar:/usr/lib/spark/jars/commons-pool-1.5.4.jar:/usr/lib/spark/jars/commons-text-1.10.0.jar:/usr/lib/spark/jars/compress-lzf-1.1.2.jar:/usr/lib/spark/jars/curator-client-2.13.0.jar:/usr/lib/spark/jars/curator-framework-2.13.0.jar:/usr/lib/spark/jars/curator-recipes-2.13.0.jar:/usr/lib/spark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/lib/spark/jars/datanucleus-core-4.1.17.jar:/usr/lib/spark/jars/datanucleus-rdbms-4.1.19.jar:/usr/lib/spark/jars/datasketches-java-3.3.0.jar:/usr/lib/spark/jars/datasketches-memory-2.1.0.jar:/usr/lib/spark/jars/derby-10.14.2.0.jar:/usr/lib/spark/jars/disruptor-3.3.7.jar:/usr/lib/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/lib/spark/jars/emr-serverless-goodies-common.jar:/usr/lib/spark/jars/emr-serverless-spark-goodies.jar:/usr/lib/spark/jars/emr-spark-goodies.jar:/usr/lib/spark/jars/error_prone_annotations-2.18.0.jar:/usr/lib/spark/jars/flatbuffers-java-1.12.0.jar:/usr/lib/spark/jars/glue-spark-goodies-2.15.0.jar:/usr/lib/spark/jars/gmetric4j-1.0.10.jar:/usr/lib/spark/jars/grpc-api-1.56.0.jar:/usr/lib/spark/jars/grpc-context-1.56.0.jar:/usr/lib/spark/jars/grpc-core-1.56.0.jar:/usr/lib/spark/jars/grpc-netty-1.56.0.jar:/usr/lib/spark/jars/grpc-protobuf-1.56.0.jar:/usr/lib/spark/jars/grpc-protobuf-lite-1.56.0.jar:/usr/lib/spark/jars/grpc-services-1.56.0.jar:/usr/lib/spark/jars/grpc-stub-1.56.0.jar:/usr/lib/spark/jars/gson-2.10.1.jar:/usr/lib/spark/jars/guava-14.0.1.jar:/usr/lib/spark/jars/hadoop-client-api-3.4.0-amzn-1.jar:/usr/lib/spark/jars/hadoop-client-runtime-3.4.0-amzn-1.jar:/usr/lib/spark/jars/hadoop-shaded-guava-1.2.0.jar:/usr/lib/spark/jars/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar:/usr/lib/spark/jars/hive-beeline-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-cli-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-common-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-exec-2.3.9-amzn-4-core.jar:/usr/lib/spark/jars/hive-jdbc-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-llap-common-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-metastore-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-serde-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-service-rpc-3.1.3.jar:/usr/lib/spark/jars/hive-shims-0.23-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-shims-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-shims-common-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-shims-scheduler-2.3.9-amzn-4.jar:/usr/lib/spark/jars/hive-storage-api-2.8.1.jar:/usr/lib/spark/jars/hk2-api-2.6.1.jar:/usr/lib/spark/jars/hk2-locator-2.6.1.jar:/usr/lib/spark/jars/hk2-utils-2.6.1.jar:/usr/lib/spark/jars/httpcore-4.4.13.jar:/usr/lib/spark/jars/httpclient-4.5.13.jar:/usr/lib/spark/jars/istack-commons-runtime-3.0.8.jar:/usr/lib/spark/jars/ivy-2.5.1.jar:/usr/lib/spark/jars/jackson-annotations-2.15.2.jar:/usr/lib/spark/jars/jackson-core-2.15.2.jar:/usr/lib/spark/jars/jackson-core-asl-1.9.13.jar:/usr/lib/spark/jars/jackson-databind-2.15.2.jar:/usr/lib/spark/jars/jackson-dataformat-yaml-2.15.2.jar:/usr/lib/spark/jars/jackson-datatype-jsr310-2.15.2.jar:/usr/lib/spark/jars/jackson-mapper-asl-1.9.13.jar:/usr/lib/spark/jars/jackson-module-scala_2.12-2.15.2.jar:/usr/lib/spark/jars/jakarta.annotation-api-1.3.5.jar:/usr/lib/spark/jars/jakarta.inject-2.6.1.jar:/usr/lib/spark/jars/jakarta.servlet-api-4.0.3.jar:/usr/lib/spark/jars/jakarta.validation-api-2.0.2.jar:/usr/lib/spark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/lib/spark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/spark/jars/janino-3.1.9.jar:/usr/lib/spark/jars/javassist-3.29.2-GA.jar:/usr/lib/spark/jars/javax.jdo-3.2.0-m3.jar:/usr/lib/spark/jars/javax.servlet-api-3.1.0.jar:/usr/lib/spark/jars/javolution-5.5.1.jar:/usr/lib/spark/jars/jaxb-runtime-2.3.2.jar:/usr/lib/spark/jars/jcl-over-slf4j-2.0.7.jar:/usr/lib/spark/jars/jdo-api-3.0.1.jar:/usr/lib/spark/jars/jersey-client-2.40.jar:/usr/lib/spark/jars/jersey-common-2.40.jar:/usr/lib/spark/jars/jersey-container-servlet-2.40.jar:/usr/lib/spark/jars/jersey-container-servlet-core-2.40.jar:/usr/lib/spark/jars/jersey-hk2-2.40.jar:/usr/lib/spark/jars/jersey-server-2.40.jar:/usr/lib/spark/jars/jetty-rewrite-9.3.27.v20190418.jar:/usr/lib/spark/jars/jline-2.14.6.jar:/usr/lib/spark/jars/joda-time-2.12.5.jar:/usr/lib/spark/jars/jodd-core-3.5.2.jar:/usr/lib/spark/jars/jpam-1.1.jar:/usr/lib/spark/jars/json-1.8.jar:/usr/lib/spark/jars/json4s-ast_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/json4s-core_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar:/usr/lib/spark/jars/jsr305-3.0.0.jar:/usr/lib/spark/jars/jta-1.1.jar:/usr/lib/spark/jars/jul-to-slf4j-2.0.7.jar:/usr/lib/spark/jars/kryo-shaded-4.0.2.jar:/usr/lib/spark/jars/lapack-3.0.3.jar:/usr/lib/spark/jars/leveldbjni-all-1.8.jar:/usr/lib/spark/jars/libfb303-0.9.3.jar:/usr/lib/spark/jars/libthrift-0.12.0.jar:/usr/lib/spark/jars/log4j-1.2-api-2.20.0.jar:/usr/lib/spark/jars/log4j-api-2.20.0.jar:/usr/lib/spark/jars/log4j-core-2.20.0.jar:/usr/lib/spark/jars/log4j-slf4j2-impl-2.20.0.jar:/usr/lib/spark/jars/logging-interceptor-3.12.12.jar:/usr/lib/spark/jars/lz4-java-1.8.0.jar:/usr/lib/spark/jars/metrics-core-4.2.19.jar:/usr/lib/spark/jars/metrics-graphite-4.2.19.jar:/usr/lib/spark/jars/metrics-jmx-4.2.19.jar:/usr/lib/spark/jars/metrics-json-4.2.19.jar:/usr/lib/spark/jars/metrics-jvm-4.2.19.jar:/usr/lib/spark/jars/minlog-1.3.0.jar:/usr/lib/spark/jars/netty-all-4.1.100.Final.jar:/usr/lib/spark/jars/netty-buffer-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-http-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-http2-4.1.100.Final.jar:/usr/lib/spark/jars/netty-codec-socks-4.1.100.Final.jar:/usr/lib/spark/jars/netty-common-4.1.100.Final.jar:/usr/lib/spark/jars/netty-handler-4.1.100.Final.jar:/usr/lib/spark/jars/netty-handler-proxy-4.1.100.Final.jar:/usr/lib/spark/jars/netty-resolver-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/lib/spark/jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/lib/spark/jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/lib/spark/jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/lib/spark/jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/lib/spark/jars/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/lib/spark/jars/objenesis-3.3.jar:/usr/lib/spark/jars/okhttp-3.12.12.jar:/usr/lib/spark/jars/okio-1.15.0.jar:/usr/lib/spark/jars/opencsv-2.3.jar:/usr/lib/spark/jars/orc-core-1.9.4-shaded-protobuf.jar:/usr/lib/spark/jars/orc-mapreduce-1.9.4-shaded-protobuf.jar:/usr/lib/spark/jars/orc-shims-1.9.4.jar:/usr/lib/spark/jars/oro-2.0.8.jar:/usr/lib/spark/jars/osgi-resource-locator-1.0.3.jar:/usr/lib/spark/jars/paranamer-2.8.jar:/usr/lib/spark/jars/parquet-column-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-common-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-encoding-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-format-structures-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-hadoop-1.13.1-amzn-3.jar:/usr/lib/spark/jars/parquet-jackson-1.13.1-amzn-3.jar:/usr/lib/spark/jars/perfmark-api-0.26.0.jar:/usr/lib/spark/jars/pickle-1.3.jar:/usr/lib/spark/jars/proto-google-common-protos-2.17.0.jar:/usr/lib/spark/jars/py4j-0.10.9.7.jar:/usr/lib/spark/jars/remotetea-oncrpc-1.1.2.jar:/usr/lib/spark/jars/rocksdbjni-8.3.2.jar:/usr/lib/spark/jars/scala-collection-compat_2.12-2.7.0.jar:/usr/lib/spark/jars/scala-compiler-2.12.18.jar:/usr/lib/spark/jars/scala-library-2.12.18.jar:/usr/lib/spark/jars/scala-parser-combinators_2.12-2.3.0.jar:/usr/lib/spark/jars/scala-reflect-2.12.18.jar:/usr/lib/spark/jars/scala-xml_2.12-2.1.0.jar:/usr/lib/spark/jars/shims-0.9.45.jar:/usr/lib/spark/jars/slf4j-api-2.0.7.jar:/usr/lib/spark/jars/snakeyaml-2.1.jar:/usr/lib/spark/jars/snakeyaml-engine-2.6.jar:/usr/lib/spark/jars/snappy-java-1.1.10.5.jar:/usr/lib/spark/jars/spark-acl-1.0.0.jar:/usr/lib/spark/jars/spark-catalyst_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-common-utils_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-core_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-fgac_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-graphx_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-hive_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-kvstore_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-launcher_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-mllib-local_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-mllib_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-network-common_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-network-shuffle_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-repl_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-sketch_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-sql-api_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-sql_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-streaming_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-tags_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-unsafe_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spark-yarn_2.12-3.5.2-amzn-1.jar:/usr/lib/spark/jars/spire-macros_2.12-0.17.0.jar:/usr/lib/spark/jars/spire-platform_2.12-0.17.0.jar:/usr/lib/spark/jars/spire-util_2.12-0.17.0.jar:/usr/lib/spark/jars/spire_2.12-0.17.0.jar:/usr/lib/spark/jars/stax-api-1.0.1.jar:/usr/lib/spark/jars/stream-2.9.6.jar:/usr/lib/spark/jars/super-csv-2.2.0.jar:/usr/lib/spark/jars/threeten-extra-1.7.1.jar:/usr/lib/spark/jars/tink-1.9.0.jar:/usr/lib/spark/jars/transaction-api-1.1.jar:/usr/lib/spark/jars/univocity-parsers-2.9.1.jar:/usr/lib/spark/jars/volcano-client-6.7.2.jar:/usr/lib/spark/jars/volcano-model-v1beta1-6.7.2.jar:/usr/lib/spark/jars/xbean-asm9-shaded-4.23.jar:/usr/lib/spark/jars/xz-1.9.jar:/usr/lib/spark/jars/zjsonpatch-0.3.0.jar:/usr/lib/spark/jars/zookeeper-3.9.1.jar:/usr/lib/spark/jars/zookeeper-jute-3.9.1.jar:/usr/lib/spark/jars/zstd-jni-1.5.5-4.jar:/usr/lib/spark/conf:/usr/share/aws/aws-java-sdk-v2/aws-sdk-java-bundle-2.28.8.jar:/usr/share/aws/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-client-common-4.2.0.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-hive3-client-4.2.0.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-hive3-client.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client-4.2.0.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-flink-runtime-1.19-1.6.1-amzn-1.jar:/usr/share/aws/iceberg/lib/iceberg-flink-runtime.jar:/usr/share/aws/iceberg/lib/iceberg-hive-runtime-1.6.1-amzn-1.jar:/usr/share/aws/iceberg/lib/iceberg-hive3-runtime.jar:/usr/share/aws/iceberg/lib/iceberg-spark-runtime-3.5_2.12-1.6.1-amzn-1.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar:/usr/share/aws/redshift/jdbc/AwsSecretsManagerCachingJava.jar:/usr/share/aws/redshift/jdbc/AwsSecretsManagerJDBC.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC42.jar:/usr/share/aws/redshift/jdbc/aws-secretsmanager-caching-java-1.0.2.jar:/usr/share/aws/redshift/jdbc/aws-secretsmanager-jdbc-1.0.12.jar:/usr/share/aws/redshift/jdbc/redshift-jdbc42-2.1.0.29.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-avro.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-avro_2.12-3.5.2-amzn-1.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-redshift.jar:/usr/share/aws/redshift/spark-redshift/lib/spark-redshift_2.12-6.3.0-spark_3.5.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-AzureCosmos-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-AzureSQL-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-BigQuery-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-MongoDB-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-MySQL-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-OpenSearch-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Oracle-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-PostgreSQL-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-SAPHana-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-SQLServer-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Snowflake-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Teradata-1.0.jar:/usr/share/aws/glue-connectors/lib/GlueSparkConnector-Vertica-1.0.jar:/usr/share/aws/glue-pii-dependencies/lib/stanford-corenlp-3.6.0-models.jar:/usr/share/aws/datazone-openlineage-spark/lib/DataZoneOpenLineageSpark-1.0.jar:/lib/*::/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.772.jar:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/AssumeRoleAWSCredentialsProvider-1.1-SNAPSHOT.jar:/usr/share/aws/emr/emrfs/lib/animal-sniffer-annotations-1.14.jar:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/checker-qual-2.5.2.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.66.0.jar:/usr/share/aws/emr/emrfs/lib/error_prone_annotations-2.1.3.jar:/usr/share/aws/emr/emrfs/lib/findbugs-annotations-3.0.1.jar:/usr/share/aws/emr/emrfs/lib/j2objc-annotations-1.1.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.12.705.jar:/usr/share/aws/emr/emrfs/lib/jsr305-3.0.2.jar:/usr/share/aws/emr/emrfs/lib/kryo-shaded-4.0.2.jar:/usr/share/aws/emr/emrfs/lib/minlog-1.3.0.jar:/usr/share/aws/emr/emrfs/lib/objenesis-2.5.1.jar:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/hadoop/hadoop-aliyun-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-aliyun.jar:/usr/lib/hadoop/hadoop-annotations-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-annotations.jar:/usr/lib/hadoop/hadoop-archive-logs-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-archive-logs.jar:/usr/lib/hadoop/hadoop-archives-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-archives.jar:/usr/lib/hadoop/hadoop-auth-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-auth.jar:/usr/lib/hadoop/hadoop-aws-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-aws.jar:/usr/lib/hadoop/hadoop-azure-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-azure-datalake-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-azure-datalake.jar:/usr/lib/hadoop/hadoop-azure.jar:/usr/lib/hadoop/hadoop-client-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-client.jar:/usr/lib/hadoop/hadoop-common-3.4.0-amzn-1-tests.jar:/usr/lib/hadoop/hadoop-common-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-common.jar:/usr/lib/hadoop/hadoop-datajoin-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-datajoin.jar:/usr/lib/hadoop/hadoop-distcp-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-distcp.jar:/usr/lib/hadoop/hadoop-dynamometer-blockgen-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-dynamometer-blockgen.jar:/usr/lib/hadoop/hadoop-dynamometer-infra-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-dynamometer-infra.jar:/usr/lib/hadoop/hadoop-dynamometer-workload-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-dynamometer-workload.jar:/usr/lib/hadoop/hadoop-extras-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-extras.jar:/usr/lib/hadoop/hadoop-federation-balance-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-federation-balance.jar:/usr/lib/hadoop/hadoop-fs2img-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-fs2img.jar:/usr/lib/hadoop/hadoop-gridmix-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-gridmix.jar:/usr/lib/hadoop/hadoop-kafka-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-kafka.jar:/usr/lib/hadoop/hadoop-kms-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-kms.jar:/usr/lib/hadoop/hadoop-minicluster-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-minicluster.jar:/usr/lib/hadoop/hadoop-nfs-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-nfs.jar:/usr/lib/hadoop/hadoop-registry-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-registry.jar:/usr/lib/hadoop/hadoop-resourceestimator-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-resourceestimator.jar:/usr/lib/hadoop/hadoop-rumen-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-rumen.jar:/usr/lib/hadoop/hadoop-shaded-guava-1.2.0.jar:/usr/lib/hadoop/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/lib/hadoop/hadoop-sls-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-sls.jar:/usr/lib/hadoop/hadoop-streaming-3.4.0-amzn-1.jar:/usr/lib/hadoop/hadoop-streaming.jar:/etc/hadoop/conf:/usr/share/java/Hive-JSON-Serde/hive-openx-serde-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/java/Hive-JSON-Serde/json-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/json-serde-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/json-serde.jar:/usr/share/java/Hive-JSON-Serde/json-udf-1.3.8.jar:/usr/share/java/Hive-JSON-Serde/json-udf.jar:/usr/share/java/Hive-JSON-Serde/json.jar:/tmp/glue-job-13064934523853194364/extra-jars/*'), ('spark.glue.GLUE_TASK_GROUP_ID', '56bbd2cb-ce20-4cc7-b155-8ee4cf33f4d1'), ('spark.hadoop.fs.s3bfs.impl', 'org.apache.hadoop.fs.s3.S3FileSystem'), ('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'), ('spark.blacklist.decommissioning.timeout', '1h'), ('spark.sql.extensions', 'com.amazonaws.services.glue.spark.extensions.SparkGlueBookmarkExtension'), ('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions'), ('spark.authenticate.secret', 'dbc985a0-94ed-4686-bd35-e0ae9037b78e'), ('spark.hadoop.fs.s3a.committer.magic.track.commits.in.memory.enabled', 'true'), ('spark.hadoop.fs.s3.buffer.dir', '/tmp/hadoop/s3'), ('spark.history.fs.logDirectory', 'file:///var/log/spark/apps'), ('spark.sql.hive.metastore.sharedPrefixes', 'software.amazon.awssdk.services.dynamodb'), ('spark.emr-serverless.client.describe.batch.size', '100'), ('spark.jars', '\"/tmp/folder-does-not-exists/\",file:///tmp/glue-job-13064934523853194364/jars/liLJjV-aws-glue-di-package-5.0.296.jar,/tmp/glue-job-13064934523853194364/jars/yYzKlj-AwsGlueMLLibs.jar,/tmp/glue-job-13064934523853194364/jars/b5k9vW-AWSGlueISArtifact-1.0.0.jar,file:///tmp/glue-job-13064934523853194364/jars/liLJjV-aws-glue-di-package-5.0.296.jar,file:///tmp/glue-job-13064934523853194364/jars/yYzKlj-AwsGlueMLLibs.jar,file:///tmp/glue-job-13064934523853194364/jars/b5k9vW-AWSGlueISArtifact-1.0.0.jar,/glue/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,/glue/usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar,/glue/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar,/glue/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar,/glue/usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar,/glue/usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar,/usr/lib/spark/jars/datanucleus-api-jdo-4.2.4.jar,/usr/lib/spark/jars/datanucleus-core-4.1.17.jar,/usr/lib/spark/jars/datanucleus-rdbms-4.1.19.jar'), ('spark.driver.port', '43795'), ('spark.app.name', '6abe1590-24b2-489e-96b2-fe8bf106612f'), ('spark.hadoop.fs.s3n.impl', 'com.amazon.ws.emr.hadoop.fs.EmrFileSystem'), ('spark.glue.USE_PROXY', 'false'), ('spark.driver.extraJavaOptions', \"-Djava.net.preferIPv6Addresses=false -XX:OnOutOfMemoryError='kill -9 %p' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.glue.etl-enable-container-telemetry-collection', 'false'), ('spark.hadoop.fs.s3.impl', 'com.amazon.ws.emr.hadoop.fs.EmrFileSystem'), ('spark.app.initial.jar.urls', 'spark://172.36.165.171:43795/jars/datanucleus-rdbms-4.1.19.jar,spark://172.36.165.171:43795/jars/b5k9vW-AWSGlueISArtifact-1.0.0.jar,spark://172.36.165.171:43795/jars/kryo-shaded-4.0.2.jar,spark://172.36.165.171:43795/jars/minlog-1.3.0.jar,spark://172.36.165.171:43795/jars/commons-codec-1.9.jar,spark://172.36.165.171:43795/jars/livy-repl_2.12-0.8.0-incubating.jar,spark://172.36.165.171:43795/jars/datanucleus-api-jdo-4.2.4.jar,spark://172.36.165.171:43795/jars/datanucleus-core-4.1.17.jar,spark://172.36.165.171:43795/jars/liLJjV-aws-glue-di-package-5.0.296.jar,spark://172.36.165.171:43795/jars/yYzKlj-AwsGlueMLLibs.jar,spark://172.36.165.171:43795/jars/livy-core_2.12-0.8.0-incubating.jar,spark://172.36.165.171:43795/jars/objenesis-2.5.1.jar'), ('spark.pyspark.python', '/usr/bin/python3.11'), ('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.glue.enable-job-insights', 'false'), ('spark.glue.glue-jars-dir', '/tmp/glue-job-13064934523853194364/jars'), ('spark.executor.id', 'driver'), ('spark.hadoop.hive.metastore.client.factory.class', 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'), ('spark.glue.glue-python-libs-dir', '/tmp/glue-job-13064934523853194364/python'), ('spark.master', 'custom:jes'), ('spark.archives', '/tmp/glue-job-13064934523853194364_glue_venv.zip#python_environment'), ('spark.dynamicAllocation.maxExecutors', '1'), ('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true'), ('spark.decommissioning.timeout.threshold', '20'), ('spark.sql.catalogImplementation', 'hive'), ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'), ('spark.files', '/etc/spark/conf.dist/hive-site.xml'), ('spark.ui.enabled', 'false'), ('spark.executor.instances', '1'), ('spark.hadoop.fs.s3.customAWSCredentialsProvider', 'com.amazonaws.auth.DefaultAWSCredentialsProviderChain'), ('spark.app.initial.file.urls', 'spark://172.36.165.171:43795/files/hive-site.xml'), ('spark.default.parallelism', '4'), ('spark.hadoop.fs.s3a.aws.credentials.provider', 'software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider'), ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'), ('spark.hadoop.fs.s3a.committer.magic.enabled', 'true'), ('spark.dynamicAllocation.enabled', 'false'), ('spark.driver.host', '172.36.165.171'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2'), ('spark.glue.additionalParams.USER_JARS_FIRST', 'false'), ('spark.glue.SESSION_ID', '6abe1590-24b2-489e-96b2-fe8bf106612f'), ('spark.authenticate', 'true'), ('spark.shuffle.service.enabled', 'false'), ('spark.yarn.submit.waitAppCompletion', 'false'), ('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/goodies/lib/emr-serverless-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar'), ('spark.app.startTime', '1736437182759'), ('spark.app.id', 'spark-application-1736437185682'), ('spark.driver.extraClassPath', '/usr/lib/livy/rsc-jars/*:/usr/lib/livy/repl_2.12-jars/*:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/goodies/lib/emr-serverless-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/iceberg/lib/iceberg-emr-common.jar:/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar'), ('spark.hadoop.fs.defaultFS', 'file:///'), ('spark.hadoop.lakeformation.credentials.url', 'http://localhost:9998/lakeformationcredentials'), ('spark.submit.customResourceManager.submit.class', 'org.apache.spark.deploy.emrserverless.submit.EmrServerlessClientApplication'), ('spark.executor.cores', '4'), ('spark.hadoop.fs.s3a.committer.name', 'magicv2'), ('spark.yarn.maxAppAttempts', '1'), ('spark.dynamicAllocation.initialExecutors', '3'), ('spark.glue.additionalParams.ADDITIONAL_GLUE_JDK_OPTS', '-Xmx10g -XX:+UseG1GC -XX:MaxHeapFreeRatio=70 -XX:InitiatingHeapOccupancyPercent=45'), ('spark.submit.deployMode', 'client'), ('spark.emr-serverless.client.create.batch.size', '100'), ('spark.ui.custom.executor.log.url', '/logs/{{CONTAINER_ID}}/{{FILE_NAME}}.gz'), ('spark.glue.glue-libs-temp-dir-path', '/tmp/glue-job-13064934523853194364'), ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'), ('spark.driver.bindAddress', '172.36.165.171'), ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'), ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true'), ('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.livy.spark_major_version', '3'), ('spark.history.ui.port', '18080'), ('spark.repl.class.uri', 'spark://172.36.165.171:43795/classes'), ('spark.executor.memory', '10g'), ('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.glue.GLUE_VERSION', '5.0'), ('spark.resourceManager.cleanupExpiredHost', 'true'), ('spark.driver.memory', '10g'), ('spark.glue.GLUE_COMMAND_CRITERIA', 'glueetl'), ('spark.glue.additionalParams.GLUE_LIBS_TEMP_DIR_PATH', '/tmp/glue-job-13064934523853194364/'), ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'), ('spark.repl.class.outputDir', '/tmp/spark6202058176104247747'), ('spark.sql.warehouse.dir', 'file:/tmp/spark-warehouse'), ('spark.executorEnv.PYTHONPATH', 'python_environment'), ('spark.app.initial.archive.urls', 'spark://172.36.165.171:43795/files/glue-job-13064934523853194364_glue_venv.zip#python_environment'), ('spark.submit.pyFiles', ''), ('spark.yarn.isPython', 'true'), ('spark.glue.additionalParams.PROXY_DISABLED_V2', 'false'), ('spark.emr-serverless.client.release.batch.size', '100'), ('spark.blacklist.decommissioning.enabled', 'true')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Import libraries",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport datetime",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Spark's Unified Framework",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "data = [('Alice', 34), ('Bob', 45), ('Cathy', 29), ('David', 50), ('Eve', 28), ('Frank', 20), ('Grace', 42), ('Hank', 21), ('Ivy', 26), ('Jack', 40), ('Karen', 19), ('Leo', 29), ('Mona', 35), ('Nina', 48), ('Javier', 38)]\ncolumns = ['Name', 'Age']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# DataFrames Pyspark\ndf = spark.createDataFrame(data, columns)\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+---+\n|  Name|Age|\n+------+---+\n| Alice| 34|\n|   Bob| 45|\n| Cathy| 29|\n| David| 50|\n|   Eve| 28|\n| Frank| 20|\n| Grace| 42|\n|  Hank| 21|\n|   Ivy| 26|\n|  Jack| 40|\n| Karen| 19|\n|   Leo| 29|\n|  Mona| 35|\n|  Nina| 48|\n|Javier| 38|\n+------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df.rdd.glom().collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "[[Row(Name='Alice', Age=34), Row(Name='Bob', Age=45), Row(Name='Cathy', Age=29)], [Row(Name='David', Age=50), Row(Name='Eve', Age=28), Row(Name='Frank', Age=20)], [Row(Name='Grace', Age=42), Row(Name='Hank', Age=21), Row(Name='Ivy', Age=26)], [Row(Name='Jack', Age=40), Row(Name='Karen', Age=19), Row(Name='Leo', Age=29), Row(Name='Mona', Age=35), Row(Name='Nina', Age=48), Row(Name='Javier', Age=38)]]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.filter(F.col('Age') > 30).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+---+\n|  Name|Age|\n+------+---+\n| Alice| 34|\n|   Bob| 45|\n| David| 50|\n| Grace| 42|\n|  Jack| 40|\n|  Mona| 35|\n|  Nina| 48|\n|Javier| 38|\n+------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# DataFrames SQL\ndf.createOrReplaceTempView('friends')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\n    '''\n    SELECT *\n    FROM friends\n    WHERE Age > 30\n    ''').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+---+\n|  Name|Age|\n+------+---+\n| Alice| 34|\n|   Bob| 45|\n| David| 50|\n| Grace| 42|\n|  Jack| 40|\n|  Mona| 35|\n|  Nina| 48|\n|Javier| 38|\n+------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# RDDs\nrdd = spark.sparkContext.parallelize(data)\nrdd.collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "[('Alice', 34), ('Bob', 45), ('Cathy', 29), ('David', 50), ('Eve', 28), ('Frank', 20), ('Grace', 42), ('Hank', 21), ('Ivy', 26), ('Jack', 40), ('Karen', 19), ('Leo', 29), ('Mona', 35), ('Nina', 48), ('Javier', 38)]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_rdd = rdd.filter(lambda x: x[1] > 30)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_rdd.collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "[('Alice', 34), ('Bob', 45), ('David', 50), ('Grace', 42), ('Jack', 40), ('Mona', 35), ('Nina', 48), ('Javier', 38)]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### RDDs - Example 1",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "this_is_a_variable = [i for i in range(10**5)]\nthis_is_a_variable[:10]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "rdd2 = spark.sparkContext.parallelize(this_is_a_variable)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "rdd2.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# l = rdd2.glom().collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_rdd = rdd2.filter(lambda x: x > 10**6 - 100)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_rdd.collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "[999901, 999902, 999903, 999904, 999905, 999906, 999907, 999908, 999909, 999910, 999911, 999912, 999913, 999914, 999915, 999916, 999917, 999918, 999919, 999920, 999921, 999922, 999923, 999924, 999925, 999926, 999927, 999928, 999929, 999930, 999931, 999932, 999933, 999934, 999935, 999936, 999937, 999938, 999939, 999940, 999941, 999942, 999943, 999944, 999945, 999946, 999947, 999948, 999949, 999950, 999951, 999952, 999953, 999954, 999955, 999956, 999957, 999958, 999959, 999960, 999961, 999962, 999963, 999964, 999965, 999966, 999967, 999968, 999969, 999970, 999971, 999972, 999973, 999974, 999975, 999976, 999977, 999978, 999979, 999980, 999981, 999982, 999983, 999984, 999985, 999986, 999987, 999988, 999989, 999990, 999991, 999992, 999993, 999994, 999995, 999996, 999997, 999998, 999999]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_rdd.glom().collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "[[], [], [], [999901, 999902, 999903, 999904, 999905, 999906, 999907, 999908, 999909, 999910, 999911, 999912, 999913, 999914, 999915, 999916, 999917, 999918, 999919, 999920, 999921, 999922, 999923, 999924, 999925, 999926, 999927, 999928, 999929, 999930, 999931, 999932, 999933, 999934, 999935, 999936, 999937, 999938, 999939, 999940, 999941, 999942, 999943, 999944, 999945, 999946, 999947, 999948, 999949, 999950, 999951, 999952, 999953, 999954, 999955, 999956, 999957, 999958, 999959, 999960, 999961, 999962, 999963, 999964, 999965, 999966, 999967, 999968, 999969, 999970, 999971, 999972, 999973, 999974, 999975, 999976, 999977, 999978, 999979, 999980, 999981, 999982, 999983, 999984, 999985, 999986, 999987, 999988, 999989, 999990, 999991, 999992, 999993, 999994, 999995, 999996, 999997, 999998, 999999]]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### RDDs - Example 2",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Lazy Transformation\ntime_to_retirement_rdd = rdd.map(lambda x: 67 - x[1])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "time_to_retirement_rdd.collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "[33, 22, 38, 17, 39, 47, 25, 46, 41, 27, 48, 38, 32, 19, 29]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "rdd.glom().collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "[[('Alice', 34), ('Bob', 45), ('Cathy', 29)], [('David', 50), ('Eve', 28), ('Frank', 20)], [('Grace', 42), ('Hank', 21), ('Ivy', 26)], [('Jack', 40), ('Karen', 19), ('Leo', 29), ('Mona', 35), ('Nina', 48), ('Javier', 38)]]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "time_to_retirement_rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "time_to_retirement_rdd.glom().collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "[[33, 22, 38], [17, 39, 47], [25, 46, 41], [27, 48, 38, 32, 19, 29]]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Lazy vs eager transformations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+---+\n|  Name|Age|\n+------+---+\n| Alice| 34|\n|   Bob| 45|\n| Cathy| 29|\n| David| 50|\n|   Eve| 28|\n| Frank| 20|\n| Grace| 42|\n|  Hank| 21|\n|   Ivy| 26|\n|  Jack| 40|\n| Karen| 19|\n|   Leo| 29|\n|  Mona| 35|\n|  Nina| 48|\n|Javier| 38|\n+------+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "8\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2 = df.withColumn('retirement_in', 67 - F.col('Age'))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df3 = df2.withColumn('older_than_30', F.when(F.col('Age')>30, F.lit(True)).otherwise(F.lit(False)))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df3.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+---+-------------+-------------+\n|  Name|Age|retirement_in|older_than_30|\n+------+---+-------------+-------------+\n| Alice| 34|           33|         true|\n|   Bob| 45|           22|         true|\n| Cathy| 29|           38|        false|\n| David| 50|           17|         true|\n|   Eve| 28|           39|        false|\n| Frank| 20|           47|        false|\n| Grace| 42|           25|         true|\n|  Hank| 21|           46|        false|\n|   Ivy| 26|           41|        false|\n|  Jack| 40|           27|         true|\n| Karen| 19|           48|        false|\n|   Leo| 29|           38|        false|\n|  Mona| 35|           32|         true|\n|  Nina| 48|           19|         true|\n|Javier| 38|           29|         true|\n+------+---+-------------+-------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df3.explain()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Physical Plan ==\n*(1) Project [Name#0, Age#1L, (67 - Age#1L) AS retirement_in#46L, ((Age#1L > 30) <=> true) AS older_than_30#85]\n+- *(1) Scan ExistingRDD[Name#0,Age#1L]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df4 = df3.groupBy('older_than_30').agg(F.count('Name').alias('total_people'), F.mean('Age').alias('mean_age'), F.stddev('Age').alias('stddev_age'))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df4.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------+------------+------------------+-----------------+\n|older_than_30|total_people|          mean_age|       stddev_age|\n+-------------+------------+------------------+-----------------+\n|         true|           8|              41.5|5.855400437691199|\n|        false|           7|24.571428571428573|4.429339411136566|\n+-------------+------------+------------------+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2.rdd.getNumPartitions()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "8\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df4.explain(True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "== Parsed Logical Plan ==\n'Aggregate ['older_than_30], ['older_than_30, count('Name) AS total_people#129, avg('Age) AS mean_age#131, 'stddev('Age) AS stddev_age#132]\n+- Project [Name#0, Age#1L, retirement_in#46L, CASE WHEN (Age#1L > cast(30 as bigint)) THEN true ELSE false END AS older_than_30#85]\n   +- Project [Name#0, Age#1L, (cast(67 as bigint) - Age#1L) AS retirement_in#46L]\n      +- LogicalRDD [Name#0, Age#1L], false\n\n== Analyzed Logical Plan ==\nolder_than_30: boolean, total_people: bigint, mean_age: double, stddev_age: double\nAggregate [older_than_30#85], [older_than_30#85, count(Name#0) AS total_people#129L, avg(Age#1L) AS mean_age#131, stddev(cast(Age#1L as double)) AS stddev_age#132]\n+- Project [Name#0, Age#1L, retirement_in#46L, CASE WHEN (Age#1L > cast(30 as bigint)) THEN true ELSE false END AS older_than_30#85]\n   +- Project [Name#0, Age#1L, (cast(67 as bigint) - Age#1L) AS retirement_in#46L]\n      +- LogicalRDD [Name#0, Age#1L], false\n\n== Optimized Logical Plan ==\nAggregate [older_than_30#85], [older_than_30#85, count(Name#0) AS total_people#129L, avg(Age#1L) AS mean_age#131, stddev(cast(Age#1L as double)) AS stddev_age#132]\n+- Project [Name#0, Age#1L, ((Age#1L > 30) <=> true) AS older_than_30#85]\n   +- LogicalRDD [Name#0, Age#1L], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[older_than_30#85], functions=[count(Name#0), avg(Age#1L), stddev(cast(Age#1L as double))], output=[older_than_30#85, total_people#129L, mean_age#131, stddev_age#132], schema specialized)\n   +- Exchange hashpartitioning(older_than_30#85, 8), ENSURE_REQUIREMENTS, [plan_id=204]\n      +- HashAggregate(keys=[older_than_30#85], functions=[partial_count(Name#0), partial_avg(Age#1L), partial_stddev(cast(Age#1L as double))], output=[older_than_30#85, count#183L, sum#186, count#187L, n#155, avg#156, m2#157], schema specialized)\n         +- Project [Name#0, Age#1L, ((Age#1L > 30) <=> true) AS older_than_30#85]\n            +- Scan ExistingRDD[Name#0,Age#1L]\n",
					"output_type": "stream"
				}
			]
		}
	]
}